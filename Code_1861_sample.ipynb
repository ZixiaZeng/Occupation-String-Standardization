{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create other year sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "OCCs_1911 = pd.read_csv(\n",
    "    \"D:\\\\Postgraduate\\\\Data Science Project\\\\Data\\\\OCCs_1851.csv\",\n",
    "    sep=\",\",  \n",
    "    header=0, \n",
    "    encoding='latin1',  \n",
    "    on_bad_lines='skip'  \n",
    ")\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    return text.strip()\n",
    "OCCs_1911['Occupation_String'] = OCCs_1911['Occupation_String'].astype(str)\n",
    "OCCs_1911['Occupation_String'] = OCCs_1911['Occupation_String'].apply(clean_text)\n",
    "\n",
    "OCCs_1911 = OCCs_1911.dropna(subset=['Occupation_String'])\n",
    "\n",
    "OCCs_1911 = OCCs_1911[OCCs_1911['Occupation_String'].str.strip() != '']\n",
    "\n",
    "Occ_HISCO = pd.read_excel(\"D:\\\\Postgraduate\\\\Data Science Project\\\\Data\\\\OCCODE_HISCO (FINAL)_ZZ.xlsx\",header=0)\n",
    "\n",
    "mapping_df = Occ_HISCO.rename(columns={\n",
    "    \"OCCODE\": \"Occode\",\n",
    "    \"OCCUPATION\": \"Occode_Desc\",\n",
    "    \"HISCO_TEXT\": \"HISCO_Desc\"\n",
    "})\n",
    "\n",
    "# only keep necessary columns\n",
    "mapping_df = mapping_df[[\"Occode\", \"HISCO\", \"Occode_Desc\", \"HISCO_Desc\"]]\n",
    "\n",
    "# use Occode only to merge\n",
    "OCCs_1911_text = OCCs_1911.merge(mapping_df, on=[\"Occode\"], how=\"left\")\n",
    "\n",
    "OCCs_1911_text['Sex'] = OCCs_1911_text['Sex'].replace({\n",
    "    'F': 'Female',\n",
    "    'M': 'Male',\n",
    "    'U': 'Unknown'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\51591\\AppData\\Local\\Temp\\ipykernel_36192\\4025218559.py:120: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  pattern_data = df[df['Occupation_String'].str.contains(pattern, case=False, na=False)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== SAMPLE QUALITY ANALYSIS ===\n",
      "\n",
      "Original dataset size: 1,725,688\n",
      "Sample size: 1,000\n",
      "Sample rate: 0.06%\n",
      "\n",
      "Sex Distribution:\n",
      "  Male: Original 68.3% vs Sample 67.1%\n",
      "  Female: Original 29.8% vs Sample 30.9%\n",
      "  Unknown: Original 1.9% vs Sample 2.0%\n",
      "\n",
      "Occupation String Complexity:\n",
      "  unique_occupations: Original 747,171 vs Sample 977\n",
      "  avg_length: Original 19.7 vs Sample 18.9\n",
      "  missing_values: Original 0 vs Sample 0\n",
      "  single_word: Original 127,053 vs Sample 136\n",
      "  multi_word: Original 1,598,635 vs Sample 864\n",
      "\n",
      "Top 10 Most Common Occupations Coverage:\n",
      "  'farmer of  acres': 0/7337 (0.0% coverage)\n",
      "  'farmer of  acres employing  labourers': 2/6125 (0.0% coverage)\n",
      "  'farmer  acres': 3/4363 (0.1% coverage)\n",
      "  'farmer of  acres employing  lab': 2/3483 (0.1% coverage)\n",
      "  'farmer of  acres employing  labourer': 2/1936 (0.1% coverage)\n",
      "  'farmer of  acres employing  men': 0/1669 (0.0% coverage)\n",
      "  'farmer  acres  lab': 0/1452 (0.0% coverage)\n",
      "  'farmer  acres  labourers': 0/1406 (0.0% coverage)\n",
      "  'farmer  acres employing  labourers': 0/1136 (0.0% coverage)\n",
      "  'farmer of  acres of land': 0/1134 (0.0% coverage)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "def extract_occupation_sample(df, sample_size=1000, random_state=42):\n",
    "    \"\"\"\n",
    "    Extract a representative sample for occupation string standardization.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with occupation data\n",
    "    - sample_size: Target sample size\n",
    "    - random_state: Random seed for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with sampled records\n",
    "    \"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    # 1. STRATIFIED SAMPLING BY KEY DIMENSIONS\n",
    "    stratified_samples = []\n",
    "    \n",
    "    # Sample by Sex to ensure gender representation\n",
    "    sex_groups = df.groupby('Sex')\n",
    "    sex_proportions = df['Sex'].value_counts(normalize=True)\n",
    "    \n",
    "    for sex, proportion in sex_proportions.items():\n",
    "        sex_sample_size = max(1, int(sample_size * 0.3 * proportion))\n",
    "        sex_data = sex_groups.get_group(sex)\n",
    "        if len(sex_data) >= sex_sample_size:\n",
    "            stratified_samples.append(sex_data.sample(n=sex_sample_size, random_state=random_state))\n",
    "        else:\n",
    "            stratified_samples.append(sex_data)\n",
    "    \n",
    "    # 2. OCCUPATION STRING COMPLEXITY SAMPLING\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Categorize occupation strings by complexity\n",
    "    def categorize_occupation_complexity(occupation_str):\n",
    "        if pd.isna(occupation_str):\n",
    "            return 'missing'\n",
    "        \n",
    "        occupation_str = str(occupation_str).lower().strip()\n",
    "        \n",
    "        # Simple single words\n",
    "        if len(occupation_str.split()) == 1:\n",
    "            return 'simple'\n",
    "        \n",
    "        # Contains numbers or special characters (potentially complex)\n",
    "        if re.search(r'\\d|[^\\w\\s]', occupation_str):\n",
    "            return 'complex'\n",
    "        \n",
    "        # Multiple words but straightforward\n",
    "        if len(occupation_str.split()) <= 3:\n",
    "            return 'moderate'\n",
    "        \n",
    "        # Long descriptions\n",
    "        return 'complex'\n",
    "    \n",
    "    df_copy['complexity'] = df_copy['Occupation_String'].apply(categorize_occupation_complexity)\n",
    "    \n",
    "    # Sample from each complexity category\n",
    "    complexity_samples = []\n",
    "    complexity_proportions = {'simple': 0.4, 'moderate': 0.3, 'complex': 0.2, 'missing': 0.1}\n",
    "    \n",
    "    for complexity, target_prop in complexity_proportions.items():\n",
    "        complexity_data = df_copy[df_copy['complexity'] == complexity]\n",
    "        if len(complexity_data) > 0:\n",
    "            complexity_sample_size = max(1, int(sample_size * 0.3 * target_prop))\n",
    "            if len(complexity_data) >= complexity_sample_size:\n",
    "                complexity_samples.append(complexity_data.sample(n=complexity_sample_size, random_state=random_state))\n",
    "            else:\n",
    "                complexity_samples.append(complexity_data)\n",
    "    \n",
    "    # 3. FREQUENCY-BASED SAMPLING\n",
    "    # Include both common and rare occupation strings\n",
    "    occupation_counts = df['Occupation_String'].value_counts()\n",
    "    \n",
    "    # High frequency occupations (top 10%)\n",
    "    high_freq_threshold = occupation_counts.quantile(0.9)\n",
    "    high_freq_occupations = occupation_counts[occupation_counts >= high_freq_threshold].index\n",
    "    high_freq_data = df[df['Occupation_String'].isin(high_freq_occupations)]\n",
    "    high_freq_sample = high_freq_data.sample(n=min(len(high_freq_data), int(sample_size * 0.2)), \n",
    "                                           random_state=random_state)\n",
    "    \n",
    "    # Low frequency occupations (bottom 50%)\n",
    "    low_freq_threshold = occupation_counts.median()\n",
    "    low_freq_occupations = occupation_counts[occupation_counts <= low_freq_threshold].index\n",
    "    low_freq_data = df[df['Occupation_String'].isin(low_freq_occupations)]\n",
    "    low_freq_sample = low_freq_data.sample(n=min(len(low_freq_data), int(sample_size * 0.2)), \n",
    "                                         random_state=random_state)\n",
    "    \n",
    "    # 4. GEOGRAPHIC DIVERSITY\n",
    "    # Sample across different counties if available\n",
    "    county_samples = []\n",
    "    if 'County' in df.columns:\n",
    "        counties = df['County'].unique()\n",
    "        county_sample_size = max(1, int(sample_size * 0.1 / len(counties)))\n",
    "        \n",
    "        for county in counties[:10]:  # Limit to top 10 counties to avoid too much fragmentation\n",
    "            county_data = df[df['County'] == county]\n",
    "            if len(county_data) >= county_sample_size:\n",
    "                county_samples.append(county_data.sample(n=county_sample_size, random_state=random_state))\n",
    "    \n",
    "    # 5. SPECIAL CASES SAMPLING\n",
    "    special_cases = []\n",
    "    \n",
    "    # Occupations with specific patterns that might need special handling\n",
    "    patterns = {\n",
    "        'family_relations': r'\\b(wife|husband|son|daughter|mother|father)\\b',\n",
    "        'apprentices': r'\\bapprentice\\b',\n",
    "        'servants': r'\\bservant\\b',\n",
    "        'farmers': r'\\bfarm\\b',\n",
    "        'makers': r'\\bmaker\\b',\n",
    "        'numerical': r'\\d+',\n",
    "        'modifiers': r'\\b(former|ex|retired|late)\\b'\n",
    "    }\n",
    "    \n",
    "    for pattern_name, pattern in patterns.items():\n",
    "        pattern_data = df[df['Occupation_String'].str.contains(pattern, case=False, na=False)]\n",
    "        if len(pattern_data) > 0:\n",
    "            pattern_sample_size = min(len(pattern_data), max(1, int(sample_size * 0.05)))\n",
    "            special_cases.append(pattern_data.sample(n=pattern_sample_size, random_state=random_state))\n",
    "    \n",
    "    # 6. COMBINE ALL SAMPLES AND DEDUPLICATE\n",
    "    all_samples = (stratified_samples + complexity_samples + [high_freq_sample, low_freq_sample] + \n",
    "                  county_samples + special_cases)\n",
    "    \n",
    "    # Combine all samples\n",
    "    combined_sample = pd.concat(all_samples, ignore_index=True)\n",
    "    \n",
    "    # Remove duplicates while preserving diversity\n",
    "    combined_sample = combined_sample.drop_duplicates(subset=['Year', 'County', 'Sex', 'Occupation_String', 'Occode'])\n",
    "    \n",
    "    # If we have more than target, randomly sample down\n",
    "    if len(combined_sample) > sample_size:\n",
    "        final_sample = combined_sample.sample(n=sample_size, random_state=random_state)\n",
    "    else:\n",
    "        # If we need more, add random samples from remaining data\n",
    "        remaining_data = df[~df.index.isin(combined_sample.index)]\n",
    "        additional_needed = sample_size - len(combined_sample)\n",
    "        \n",
    "        if len(remaining_data) >= additional_needed:\n",
    "            additional_sample = remaining_data.sample(n=additional_needed, random_state=random_state)\n",
    "            final_sample = pd.concat([combined_sample, additional_sample], ignore_index=True)\n",
    "        else:\n",
    "            final_sample = combined_sample\n",
    "    \n",
    "    return final_sample.reset_index(drop=True)\n",
    "\n",
    "def analyze_sample_quality(original_df, sample_df):\n",
    "    \"\"\"\n",
    "    Analyze the quality and representativeness of the sample.\n",
    "    \"\"\"\n",
    "    print(\"=== SAMPLE QUALITY ANALYSIS ===\\n\")\n",
    "    \n",
    "    print(f\"Original dataset size: {len(original_df):,}\")\n",
    "    print(f\"Sample size: {len(sample_df):,}\")\n",
    "    print(f\"Sample rate: {len(sample_df)/len(original_df)*100:.2f}%\\n\")\n",
    "    \n",
    "    # Sex distribution comparison\n",
    "    print(\"Sex Distribution:\")\n",
    "    orig_sex = original_df['Sex'].value_counts(normalize=True)\n",
    "    sample_sex = sample_df['Sex'].value_counts(normalize=True)\n",
    "    \n",
    "    for sex in orig_sex.index:\n",
    "        orig_pct = orig_sex.get(sex, 0) * 100\n",
    "        sample_pct = sample_sex.get(sex, 0) * 100\n",
    "        print(f\"  {sex}: Original {orig_pct:.1f}% vs Sample {sample_pct:.1f}%\")\n",
    "    \n",
    "    # Occupation string complexity analysis\n",
    "    def get_complexity_stats(df):\n",
    "        stats = {\n",
    "            'unique_occupations': df['Occupation_String'].nunique(),\n",
    "            'avg_length': df['Occupation_String'].str.len().mean(),\n",
    "            'missing_values': df['Occupation_String'].isna().sum(),\n",
    "            'single_word': df['Occupation_String'].str.split().str.len().eq(1).sum(),\n",
    "            'multi_word': df['Occupation_String'].str.split().str.len().gt(1).sum()\n",
    "        }\n",
    "        return stats\n",
    "    \n",
    "    print(f\"\\nOccupation String Complexity:\")\n",
    "    orig_stats = get_complexity_stats(original_df)\n",
    "    sample_stats = get_complexity_stats(sample_df)\n",
    "    \n",
    "    for metric, orig_val in orig_stats.items():\n",
    "        sample_val = sample_stats[metric]\n",
    "        if metric == 'avg_length':\n",
    "            print(f\"  {metric}: Original {orig_val:.1f} vs Sample {sample_val:.1f}\")\n",
    "        else:\n",
    "            print(f\"  {metric}: Original {orig_val:,} vs Sample {sample_val:,}\")\n",
    "    \n",
    "    # Top occupations coverage\n",
    "    print(f\"\\nTop 10 Most Common Occupations Coverage:\")\n",
    "    top_occupations = original_df['Occupation_String'].value_counts().head(10)\n",
    "    \n",
    "    for occupation, count in top_occupations.items():\n",
    "        sample_count = (sample_df['Occupation_String'] == occupation).sum()\n",
    "        coverage = sample_count / count * 100 if count > 0 else 0\n",
    "        print(f\"  '{occupation}': {sample_count}/{count} ({coverage:.1f}% coverage)\")\n",
    "\n",
    "# Example usage:\n",
    "# df = pd.read_csv('your_1861_occupation_data.csv')\n",
    "sample = extract_occupation_sample(OCCs_1911_text, sample_size=1000)\n",
    "analyze_sample_quality(OCCs_1911_text, sample)\n",
    "sample.to_csv('occupation_sample_for_standardization_1851.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-based data pre-cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\hisco\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Anaconda\\envs\\hisco\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] 找不到指定的程序。'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK resources loaded successfully\n",
      "Loaded 1000 records from 56 counties.\n",
      "Processing in chunks of 1000 per county...\n",
      "Dataset: 1000 records, 977 unique occupations\n",
      "Counties: 56\n",
      "Loaded 234377 English words from NLTK\n",
      "Loaded dictionaries from occupation_dictionaries.xlsx\n",
      "  - 235 abbreviations\n",
      "  - 84 misspellings\n",
      "  - 17 categories\n",
      "Processing 1000 records in chunks of 1000...\n",
      "Multi-occupation detection: Simple flagging enabled\n",
      "Standardizing occupations...\n",
      "Standardizing 977 unique occupations...\n",
      "Multi-occupation detection enabled (simple flagging)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing occupation chunks: 100%|██████████| 1/1 [00:03<00:00,  3.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging results with original DataFrame by county...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing counties: 100%|██████████| 56/56 [00:00<00:00, 873.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ENHANCED PROCESSING SUMMARY ===\n",
      "Total records processed: 1,000\n",
      "Records needing LLM check: 189 (18.9%)\n",
      "Average confidence score: 0.768\n",
      "Multi-occupation records detected: 30 (3.0%)\n",
      "\n",
      "Top reasons for LLM check:\n",
      "  acceptable_quality: 771 (77.1%)\n",
      "  unchanged_with_some_invalid_words: 95 (9.5%)\n",
      "  unchanged_with_many_invalid_words: 50 (5.0%)\n",
      "  modification_poor_result: 34 (3.4%)\n",
      "  multiple_occupation_excluded_from_llm_check: 30 (3.0%)\n",
      "  unchanged_with_few_invalid_words: 10 (1.0%)\n",
      "  low_confidence: 10 (1.0%)\n",
      "Exported main dataset to ./cleaning_results_1851_validation\\cleaned_occupations_with_categories.csv\n",
      "Exported LLM check subset to ./cleaning_results_1851_validation\\llm_check_needed.csv (189 records)\n",
      "Exported multi-occupation flagged records to ./cleaning_results_1851_validation\\multi_occupation_flagged.csv (30 records)\n",
      "\n",
      "Processing completed! Cleaned 1000 records.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from rapidfuzz import fuzz, process\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import re\n",
    "import string\n",
    "from typing import Dict, List, Tuple, Set\n",
    "import difflib\n",
    "import os\n",
    "import gc\n",
    "import math\n",
    "\n",
    "# English dictionary validation setup\n",
    "try:\n",
    "    import nltk\n",
    "    from nltk.corpus import wordnet, words\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    \n",
    "    # Download required NLTK data\n",
    "    for resource in ['wordnet', 'words', 'averaged_perceptron_tagger']:\n",
    "        try:\n",
    "            nltk.data.find(f'corpora/{resource}' if resource != 'averaged_perceptron_tagger' else f'taggers/{resource}')\n",
    "        except LookupError:\n",
    "            nltk.download(resource, quiet=True)\n",
    "    \n",
    "    NLTK_AVAILABLE = True\n",
    "    print(\"NLTK resources loaded successfully\")\n",
    "except ImportError:\n",
    "    print(\"NLTK not available. Using basic dictionary validation.\")\n",
    "    NLTK_AVAILABLE = False\n",
    "\n",
    "# Basic English words as fallback\n",
    "BASIC_ENGLISH_WORDS = {\n",
    "    'worker', 'man', 'woman', 'boy', 'girl', 'master', 'apprentice', 'journeyman',\n",
    "    'labourer', 'laborer', 'servant', 'farmer', 'baker', 'smith', 'carpenter', \n",
    "    'weaver', 'tailor', 'shoemaker', 'blacksmith', 'agricultural', 'domestic',\n",
    "    'general', 'coal', 'iron', 'cotton', 'wool', 'silk', 'wife', 'widow', \n",
    "    'daughter', 'son', 'mother', 'father', 'sister', 'brother', 'retired',\n",
    "    'unemployed', 'former', 'late', 'head', 'assistant', 'clerk', 'teacher',\n",
    "    'nurse', 'doctor', 'merchant', 'shopkeeper', 'miner', 'miller', 'butcher',\n",
    "    'grocer', 'publican', 'innkeeper', 'coachman', 'groom', 'gardener'\n",
    "}\n",
    "\n",
    "class TraditionalOccupationCleaner:\n",
    "    def __init__(self, excel_file_path: str = None):\n",
    "        \"\"\"\n",
    "        Initialize the traditional occupation cleaner\n",
    "        Args:\n",
    "            excel_file_path: Path to Excel file containing dictionaries\n",
    "        \"\"\"\n",
    "        self.embedding_model = SentenceTransformer('sentence-transformers/all-mpnet-base-v2')\n",
    "        \n",
    "        # Initialize English dictionary validation\n",
    "        self._initialize_english_dictionary_validation()\n",
    "        \n",
    "        # NEW: Initialize multi-occupation detection patterns\n",
    "        self._initialize_multi_occupation_patterns()\n",
    "        \n",
    "        # Load dictionaries from Excel file or use empty defaults\n",
    "        if excel_file_path and os.path.exists(excel_file_path):\n",
    "            self.abbreviation_dict, self.common_misspellings, self.occupation_categories, self.important_modifiers = self.load_dictionaries_from_excel(excel_file_path)\n",
    "        else:\n",
    "            # Empty dictionaries if no file provided\n",
    "            self.abbreviation_dict = {}\n",
    "            self.common_misspellings = {}\n",
    "            self.important_modifiers = {}\n",
    "            self.occupation_categories = {\n",
    "                'agricultural': [],\n",
    "                'domestic_service': [],\n",
    "                'textile_manufacture': [],\n",
    "                'clothing_footwear': [],\n",
    "                'mining_quarrying': [],\n",
    "                'building_construction': [],\n",
    "                'metalwork_engineering': [],\n",
    "                'food_drink': [],\n",
    "                'transport_communication': [],\n",
    "                'professional_clerical': [],\n",
    "                'retail_trade': [],\n",
    "                'manufacturing_other': [],\n",
    "                'crafts_trades': [],\n",
    "                'personal_service': [],\n",
    "                'public_service': [],\n",
    "                'general_labour': [],\n",
    "                'other': []\n",
    "            }\n",
    "    \n",
    "    def _initialize_multi_occupation_patterns(self):\n",
    "        \"\"\"\n",
    "        Initialize patterns for detecting multiple occupations\n",
    "        \"\"\"\n",
    "        # Common separators and connectors in historical records\n",
    "        self.occupation_separators = [\n",
    "            r'\\s+and\\s+',           # \" and \"\n",
    "            r'\\s*&\\s*',             # \" & \" or \"&\"\n",
    "            r'\\s*\\+\\s*',            # \" + \" or \"+\"\n",
    "            r'\\s*/\\s*',             # \" / \" or \"/\"\n",
    "            r'\\s*,\\s*and\\s+',       # \", and \"\n",
    "            r'\\s+also\\s+',          # \" also \"\n",
    "            r'\\s+or\\s+',            # \" or \"\n",
    "        ]\n",
    "        \n",
    "        # Compile regex patterns for efficiency\n",
    "        self.separator_patterns = [re.compile(pattern, re.IGNORECASE) for pattern in self.occupation_separators]\n",
    "        \n",
    "        # Words that should NOT be considered as separate occupations\n",
    "        self.non_occupation_words = {\n",
    "            'wife', 'widow', 'daughter', 'son', 'mother', 'father', 'sister', 'brother',\n",
    "            'unmarried', 'spinster', 'bachelor', 'widower', 'married',\n",
    "            'unemployed', 'retired', 'former', 'late', 'ex',\n",
    "            'apprentice', 'master', 'journeyman', 'assistant', 'head',\n",
    "            'the', 'a', 'an', 'of', 'in', 'at', 'to', 'for', 'with', 'by'\n",
    "        }\n",
    "    \n",
    "    def _detect_multiple_occupations(self, text: str) -> bool:\n",
    "        \"\"\"\n",
    "        Simple detection of multiple occupations - returns True/False only\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or not text.strip():\n",
    "            return False\n",
    "        \n",
    "        normalized_text = self.normalize_text(text)\n",
    "        \n",
    "        # Check for obvious multi-occupation patterns\n",
    "        for pattern in self.separator_patterns:\n",
    "            if pattern.search(normalized_text):\n",
    "                # Split by this pattern\n",
    "                parts = pattern.split(normalized_text)\n",
    "                # Clean up parts\n",
    "                occupations = [part.strip() for part in parts if part.strip()]\n",
    "                \n",
    "                # Only consider it multiple if get 2+ meaningful parts\n",
    "                if len(occupations) >= 2:\n",
    "                    # Check that each part is substantial and looks like an occupation\n",
    "                    substantial_parts = []\n",
    "                    for part in occupations:\n",
    "                        # Skip very short parts, common words, and modifier words\n",
    "                        if (len(part) > 2 and \n",
    "                            part not in self.non_occupation_words and\n",
    "                            len(part.split()) >= 1):  # At least one word\n",
    "                            substantial_parts.append(part)\n",
    "                    \n",
    "                    # If we have 2+ substantial parts that look like occupations, it's multiple\n",
    "                    if len(substantial_parts) >= 2:\n",
    "                        return True\n",
    "        \n",
    "        # Special case: Check for comma-separated occupations (more conservative)\n",
    "        # Only if the parts look like actual occupations and don't contain family/modifier words\n",
    "        if ',' in normalized_text and 'and' not in normalized_text:\n",
    "            parts = [part.strip() for part in normalized_text.split(',')]\n",
    "            if len(parts) >= 2:\n",
    "                # Check if each part looks like an occupation (not a modifier)\n",
    "                occupation_like_parts = []\n",
    "                for part in parts:\n",
    "                    # Skip if it's likely a modifier (family relation, etc.)\n",
    "                    if (len(part) > 3 and \n",
    "                        not any(modifier in part for modifier in self.non_occupation_words) and\n",
    "                        not part in self.non_occupation_words):\n",
    "                        occupation_like_parts.append(part)\n",
    "                \n",
    "                if len(occupation_like_parts) >= 2:\n",
    "                    return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _initialize_english_dictionary_validation(self):\n",
    "        \"\"\"\n",
    "        Initialize English dictionary validation tools\n",
    "        \"\"\"\n",
    "        self.lemmatizer = None\n",
    "        self.english_words_set = set()\n",
    "        \n",
    "        if NLTK_AVAILABLE:\n",
    "            try:\n",
    "                self.lemmatizer = WordNetLemmatizer()\n",
    "                english_words = set(words.words())\n",
    "                self.english_words_set = {word.lower() for word in english_words}\n",
    "                print(f\"Loaded {len(self.english_words_set)} English words from NLTK\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading NLTK resources: {e}\")\n",
    "                self.english_words_set = BASIC_ENGLISH_WORDS.copy()\n",
    "        else:\n",
    "            self.english_words_set = BASIC_ENGLISH_WORDS.copy()\n",
    "        \n",
    "        # Add historical occupation terms\n",
    "        historical_terms = {\n",
    "            'labourer', 'laborer', 'ag', 'lab', 'serv', 'dom', 'agric', 'gen',\n",
    "            'blacksmith', 'whitesmith', 'tinsmith', 'goldsmith', 'silversmith',\n",
    "            'wheelwright', 'millwright', 'shipwright', 'cordwainer', 'ostler',\n",
    "            'victualler', 'chandler', 'draper', 'mercer', 'haberdasher',\n",
    "            'maltster', 'brewer', 'distiller', 'tanner', 'currier', 'fellmonger'\n",
    "        }\n",
    "        self.english_words_set.update(historical_terms)\n",
    "\n",
    "    def _is_valid_english_word(self, word: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check if a single word is a valid English word\n",
    "        \"\"\"\n",
    "        if not word or len(word) < 2:\n",
    "            return False\n",
    "        \n",
    "        word_lower = word.lower()\n",
    "        \n",
    "        # 1. Direct dictionary lookup\n",
    "        if word_lower in self.english_words_set:\n",
    "            return True\n",
    "        \n",
    "        # 2. NLTK wordnet check if available\n",
    "        if NLTK_AVAILABLE and self.lemmatizer:\n",
    "            try:\n",
    "                # Check lemmatized form\n",
    "                lemmatized = self.lemmatizer.lemmatize(word_lower)\n",
    "                if lemmatized in self.english_words_set:\n",
    "                    return True\n",
    "                \n",
    "                # Check wordnet synsets\n",
    "                synsets = wordnet.synsets(word_lower)\n",
    "                if synsets:\n",
    "                    return True\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # 3. Check common occupation suffixes\n",
    "        occupation_suffixes = ['er', 'or', 'ist', 'ian', 'man', 'woman', 'smith', 'wright', 'maker']\n",
    "        for suffix in occupation_suffixes:\n",
    "            if word_lower.endswith(suffix):\n",
    "                root = word_lower[:-len(suffix)]\n",
    "                if len(root) >= 3 and root in self.english_words_set:\n",
    "                    return True\n",
    "        \n",
    "        # 4. Check plural forms\n",
    "        if word_lower.endswith('s') and len(word_lower) > 3:\n",
    "            singular = word_lower[:-1]\n",
    "            if singular in self.english_words_set:\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "    def _validate_english_words(self, text: str) -> Dict[str, any]:\n",
    "        \"\"\"\n",
    "        Validate English words in text for confidence calculation\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or not text.strip():\n",
    "            return {\n",
    "                'valid_words': [],\n",
    "                'invalid_words': [],\n",
    "                'validity_ratio': 0.0,\n",
    "                'contains_invalid': True\n",
    "            }\n",
    "        \n",
    "        normalized = self.normalize_text(text)\n",
    "        words = normalized.split()\n",
    "        \n",
    "        if not words:\n",
    "            return {\n",
    "                'valid_words': [],\n",
    "                'invalid_words': [],\n",
    "                'validity_ratio': 0.0,\n",
    "                'contains_invalid': True\n",
    "            }\n",
    "        \n",
    "        valid_words = []\n",
    "        invalid_words = []\n",
    "        \n",
    "        for word in words:\n",
    "            if self._is_valid_english_word(word):\n",
    "                valid_words.append(word)\n",
    "            else:\n",
    "                invalid_words.append(word)\n",
    "        \n",
    "        validity_ratio = len(valid_words) / len(words) if words else 0.0\n",
    "        contains_invalid = len(invalid_words) > 0\n",
    "        \n",
    "        return {\n",
    "            'valid_words': valid_words,\n",
    "            'invalid_words': invalid_words,\n",
    "            'validity_ratio': validity_ratio,\n",
    "            'contains_invalid': contains_invalid\n",
    "        }\n",
    "\n",
    "    def load_dictionaries_from_excel(self, file_path: str) -> Tuple[Dict, Dict, Dict]:\n",
    "        \"\"\"\n",
    "        Load occupation dictionaries from Excel file\n",
    "        Expected sheets: 'abbreviations', 'misspellings', 'categories'\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Read Excel sheets\n",
    "            abbreviations_df = pd.read_excel(file_path, sheet_name='abbreviations')\n",
    "            misspellings_df = pd.read_excel(file_path, sheet_name='misspellings')\n",
    "            categories_df = pd.read_excel(file_path, sheet_name='categories')\n",
    "            modifiers_df = pd.read_excel(file_path, sheet_name='modifiers')\n",
    "            # Convert to dictionaries\n",
    "            abbreviation_dict = dict(zip(abbreviations_df['abbreviation'], abbreviations_df['full_form']))\n",
    "            misspellings_dict = dict(zip(misspellings_df['misspelling'], misspellings_df['correction']))\n",
    "            important_modifiers = {}\n",
    "            all_modifiers_set = set()\n",
    "            \n",
    "            if 'modifier_word' in modifiers_df.columns and 'modifier_type' in modifiers_df.columns:\n",
    "                # modifier_word, modifier_type\n",
    "                for _, row in modifiers_df.iterrows():\n",
    "                    word = row['modifier_word']\n",
    "                    mod_type = row['modifier_type']\n",
    "                    if mod_type not in important_modifiers:\n",
    "                        important_modifiers[mod_type] = []\n",
    "                    important_modifiers[mod_type].append(word)\n",
    "                    all_modifiers_set.add(word)\n",
    "            else:\n",
    "\n",
    "                word_col = None\n",
    "                type_col = None\n",
    "                \n",
    "                for col in modifiers_df.columns:\n",
    "                    if 'modifier' in col.lower() and 'type' not in col.lower():\n",
    "                        word_col = col\n",
    "                    elif 'type' in col.lower():\n",
    "                        type_col = col\n",
    "                \n",
    "                if word_col and type_col:\n",
    "                    for _, row in modifiers_df.iterrows():\n",
    "                        word = row[word_col]\n",
    "                        mod_type = row[type_col]\n",
    "                        if mod_type not in important_modifiers:\n",
    "                            important_modifiers[mod_type] = []\n",
    "                        important_modifiers[mod_type].append(word)\n",
    "                        all_modifiers_set.add(word)\n",
    "                else:\n",
    "                    print(\"Warning: Could not find modifier columns, using defaults\")\n",
    "                    important_modifiers = self._get_default_modifiers()\n",
    "                    all_modifiers_set = set()\n",
    "                    for words in important_modifiers.values():\n",
    "                        all_modifiers_set.update(words)\n",
    "            \n",
    "            self.all_modifiers_set = all_modifiers_set\n",
    "\n",
    "            # Convert categories\n",
    "            occupation_categories = {}\n",
    "            for category in categories_df['category'].unique():\n",
    "                occupations = categories_df[categories_df['category'] == category]['occupation'].tolist()\n",
    "                occupation_categories[category] = occupations\n",
    "            \n",
    "            print(f\"Loaded dictionaries from {file_path}\")\n",
    "            print(f\"  - {len(abbreviation_dict)} abbreviations\")\n",
    "            print(f\"  - {len(misspellings_dict)} misspellings\")\n",
    "            print(f\"  - {len(occupation_categories)} categories\")\n",
    "            \n",
    "            return abbreviation_dict, misspellings_dict, occupation_categories, important_modifiers\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Excel file {file_path}: {e}\")\n",
    "            print(\"Using empty dictionaries\")\n",
    "            \n",
    "            important_modifiers = self._get_default_modifiers()\n",
    "            self.all_modifiers_set = set()\n",
    "            for words in important_modifiers.values():\n",
    "                self.all_modifiers_set.update(words)\n",
    "            return {}, {}, {}, important_modifiers\n",
    "        \n",
    "    def _get_default_modifiers(self):\n",
    "        return {\n",
    "            'family_relation': ['wife', 'widow', 'daughter', 'son', 'mother', 'father', 'sister', 'brother'],\n",
    "            'marital_status': ['unmarried', 'spinster', 'bachelor', 'widower', 'married'],\n",
    "            'employment_status': ['unemployed', 'retired', 'former', 'late', 'ex'],\n",
    "            'skill_level': ['apprentice', 'master', 'journeyman', 'assistant', 'head']\n",
    "        }\n",
    "    \n",
    "    def normalize_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Normalize text by removing punctuation, extra spaces, and converting to lowercase\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text).lower().strip()\n",
    "        text = re.sub(r'[^\\w\\s\\-&]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "        text = text.strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def expand_abbreviations(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Expand common abbreviations with word boundary detection\n",
    "        \"\"\"\n",
    "        normalized_text = self.normalize_text(text)\n",
    "        \n",
    "        if not self.abbreviation_dict:\n",
    "            return normalized_text\n",
    "            \n",
    "        if normalized_text in self.abbreviation_dict:\n",
    "            return self.abbreviation_dict[normalized_text]\n",
    "        \n",
    "        sorted_abbrs = sorted(self.abbreviation_dict.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "        result_text = normalized_text\n",
    "        \n",
    "        for abbr, full_form in sorted_abbrs:\n",
    "            pattern = r'\\b' + re.escape(abbr) + r'\\b'\n",
    "            if re.search(pattern, result_text):\n",
    "                result_text = re.sub(pattern, full_form, result_text)\n",
    "        \n",
    "        return result_text\n",
    "    \n",
    "    def correct_spelling(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        FIXED: Correct common spelling errors with proper word boundary detection\n",
    "        Now correctly handles \"blacsmith wife\" → \"blacksmith wife\"\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        normalized_text = self.normalize_text(text)\n",
    "        \n",
    "        if not self.common_misspellings:\n",
    "            return normalized_text\n",
    "            \n",
    "        # Step 1: Check for exact match first (highest priority)\n",
    "        if normalized_text in self.common_misspellings:\n",
    "            return self.common_misspellings[normalized_text]\n",
    "        \n",
    "        result_text = normalized_text\n",
    "        \n",
    "        # Step 2: Apply word-level corrections\n",
    "        # Sort by length (longest first) to prevent partial matches\n",
    "        sorted_misspellings = sorted(self.common_misspellings.items(), key=lambda x: len(x[0]), reverse=True)\n",
    "        \n",
    "        for misspelling, correction in sorted_misspellings:\n",
    "            # Skip if this is a multi-word entry (handled differently)\n",
    "            if ' ' in misspelling:\n",
    "                # For multi-word misspellings, use direct string replacement\n",
    "                if misspelling in result_text:\n",
    "                    result_text = result_text.replace(misspelling, correction)\n",
    "            else:\n",
    "                # For single-word misspellings, use word boundary matching\n",
    "                pattern = r'\\b' + re.escape(misspelling) + r'\\b'\n",
    "                if re.search(pattern, result_text, re.IGNORECASE):\n",
    "                    result_text = re.sub(pattern, correction, result_text, flags=re.IGNORECASE)\n",
    "        \n",
    "        return result_text\n",
    "    \n",
    "    def fuzzy_match_occupations(self, text: str, threshold: int = 80) -> str:\n",
    "        \"\"\"\n",
    "        Use fuzzy matching to find similar standard occupations\n",
    "        \"\"\"\n",
    "        if not self.occupation_categories:\n",
    "            return text\n",
    "            \n",
    "        all_standard_occupations = []\n",
    "        for category, occupations in self.occupation_categories.items():\n",
    "            all_standard_occupations.extend(occupations)\n",
    "        \n",
    "        if not all_standard_occupations:\n",
    "            return text\n",
    "            \n",
    "        best_match = process.extractOne(\n",
    "            text, \n",
    "            all_standard_occupations, \n",
    "            scorer=fuzz.ratio,\n",
    "            score_cutoff=threshold\n",
    "        )\n",
    "        \n",
    "        if best_match:\n",
    "            return best_match[0]\n",
    "        return text\n",
    "    \n",
    "    def get_occupation_category(self, occupation: str) -> str:\n",
    "        \"\"\"\n",
    "        Determine the category of an occupation\n",
    "        \"\"\"\n",
    "        if not self.occupation_categories:\n",
    "            return 'other'\n",
    "            \n",
    "        occupation_lower = occupation.lower()\n",
    "        \n",
    "        for category, occupations in self.occupation_categories.items():\n",
    "            for occ in occupations:\n",
    "                if occ.lower() in occupation_lower or occupation_lower in occ.lower():\n",
    "                    return category\n",
    "        \n",
    "        return 'other'\n",
    "    \n",
    "    def calculate_confidence(self, Occuption_String: str, standardized: str, is_abbreviation: bool, is_misspelled: bool) -> float:\n",
    "        \"\"\"\n",
    "        ENHANCED: Multi-factor confidence estimation with English dictionary validation\n",
    "        \"\"\"\n",
    "        if pd.isna(Occuption_String) or pd.isna(standardized):\n",
    "            return 0.0\n",
    "            \n",
    "        original_clean = str(Occuption_String).lower().strip()\n",
    "        standardized_clean = str(standardized).lower().strip()\n",
    "        \n",
    "        # Check if unchanged\n",
    "        is_unchanged = original_clean == standardized_clean\n",
    "        \n",
    "        # English dictionary validation during confidence calculation\n",
    "        original_validation = self._validate_english_words(original_clean)\n",
    "        standardized_validation = self._validate_english_words(standardized_clean)\n",
    "        \n",
    "        # Calculate base confidence using original logic\n",
    "        base_confidence = self._calculate_base_confidence_original_logic(\n",
    "            original_clean, standardized_clean, is_abbreviation, is_misspelled\n",
    "        )\n",
    "        \n",
    "        # Apply dictionary-based confidence adjustments\n",
    "        final_confidence = self._apply_dictionary_confidence_adjustments(\n",
    "            base_confidence, original_validation, standardized_validation, is_unchanged\n",
    "        )\n",
    "        \n",
    "        return max(0.05, min(1.0, final_confidence))\n",
    "\n",
    "    def _calculate_base_confidence_original_logic(self, original_clean: str, standardized_clean: str, \n",
    "                                                is_abbreviation: bool, is_misspelled: bool) -> float:\n",
    "        \"\"\"\n",
    "        Calculate base confidence using original logic\n",
    "        \"\"\"\n",
    "        # Perfect match gets high confidence (but not 1.0 anymore)\n",
    "        if original_clean == standardized_clean:\n",
    "            return 0.9  # Reduced from 1.0 to allow dictionary adjustment\n",
    "        \n",
    "        # Factor 1: Semantic similarity using multiple metrics\n",
    "        token_similarity = fuzz.token_sort_ratio(original_clean, standardized_clean) / 100.0\n",
    "        partial_similarity = fuzz.partial_ratio(original_clean, standardized_clean) / 100.0\n",
    "        ratio_similarity = fuzz.ratio(original_clean, standardized_clean) / 100.0\n",
    "        \n",
    "        # Weighted combination of similarity metrics\n",
    "        semantic_score = (0.4 * token_similarity + 0.3 * partial_similarity + 0.3 * ratio_similarity)\n",
    "        \n",
    "        # Factor 2: Length ratio penalty\n",
    "        len_ratio = min(len(original_clean), len(standardized_clean)) / max(len(original_clean), len(standardized_clean))\n",
    "        length_penalty = math.exp(1 - 1/len_ratio) if len_ratio > 0 else 0.0\n",
    "        \n",
    "        # Factor 3: Transformation type confidence weights\n",
    "        transformation_confidence = 1.0\n",
    "        \n",
    "        if is_abbreviation:\n",
    "            abbr_confidence = self._assess_abbreviation_quality(original_clean, standardized_clean)\n",
    "            transformation_confidence *= abbr_confidence\n",
    "        \n",
    "        if is_misspelled:\n",
    "            spell_confidence = self._assess_spelling_correction_quality(original_clean, standardized_clean)\n",
    "            transformation_confidence *= spell_confidence\n",
    "        \n",
    "        # Factor 4: Character overlap coefficient\n",
    "        char_set_original = set(original_clean.replace(' ', ''))\n",
    "        char_set_standardized = set(standardized_clean.replace(' ', ''))\n",
    "        char_overlap = len(char_set_original & char_set_standardized) / len(char_set_original | char_set_standardized) if char_set_original | char_set_standardized else 0\n",
    "        \n",
    "        # Factor 5: Word-level semantic preservation\n",
    "        words_original = set(original_clean.split())\n",
    "        words_standardized = set(standardized_clean.split())\n",
    "        word_preservation = len(words_original & words_standardized) / len(words_original) if words_original else 0\n",
    "        \n",
    "        # Multi-factor confidence calculation using geometric mean\n",
    "        factors = [semantic_score, length_penalty, transformation_confidence, char_overlap, word_preservation]\n",
    "        non_zero_factors = [f for f in factors if f > 0]\n",
    "        \n",
    "        if not non_zero_factors:\n",
    "            return 0.1\n",
    "        \n",
    "        confidence = math.pow(math.prod(non_zero_factors), 1.0/len(non_zero_factors))\n",
    "        return confidence\n",
    "\n",
    "    def _apply_dictionary_confidence_adjustments(self, base_confidence: float, original_validation: Dict, \n",
    "                                               standardized_validation: Dict, is_unchanged: bool) -> float:\n",
    "        \"\"\"\n",
    "        Apply dictionary-based confidence adjustments\n",
    "        \"\"\"\n",
    "        original_validity = original_validation['validity_ratio']\n",
    "        standardized_validity = standardized_validation['validity_ratio']\n",
    "        \n",
    "        # Case 1: Unchanged with invalid words - Major confidence penalty\n",
    "        if is_unchanged and original_validation['contains_invalid']:\n",
    "            if original_validity < 0.5:\n",
    "                return base_confidence * 0.3  # Strong penalty for many invalid words\n",
    "            elif original_validity < 0.8:\n",
    "                return base_confidence * 0.6  # Moderate penalty for some invalid words\n",
    "            else:\n",
    "                return base_confidence * 0.8  # Light penalty for few invalid words\n",
    "        \n",
    "        # Case 2: Unchanged with all valid words - Maintain confidence\n",
    "        if is_unchanged and not original_validation['contains_invalid']:\n",
    "            return base_confidence\n",
    "        \n",
    "        # Case 3: Modified - Check if modification improved validity\n",
    "        if not is_unchanged:\n",
    "            validity_improvement = standardized_validity - original_validity\n",
    "            \n",
    "            if validity_improvement > 0.3:\n",
    "                return base_confidence * 1.1  # Bonus for significant improvement\n",
    "            elif validity_improvement > 0:\n",
    "                return base_confidence  # Neutral for moderate improvement\n",
    "            elif standardized_validity < 0.7:\n",
    "                return base_confidence * 0.7  # Penalty for poor result after modification\n",
    "            else:\n",
    "                return base_confidence * 0.9  # Small penalty for neutral modification\n",
    "        \n",
    "        return base_confidence\n",
    "    \n",
    "    def _assess_abbreviation_quality(self, original: str, standardized: str) -> float:\n",
    "        \"\"\"\n",
    "        Assess the quality of abbreviation expansion\n",
    "        High confidence for logical abbreviations, lower for poor matches\n",
    "        \"\"\"\n",
    "        # Check if abbreviation follows common patterns\n",
    "        original_words = original.split()\n",
    "        standardized_words = standardized.split()\n",
    "        \n",
    "        # Pattern 1: Initials match (e.g., \"ag lab\" -> \"agricultural labourer\")\n",
    "        if len(original_words) <= len(standardized_words):\n",
    "            initial_match_score = 0.0\n",
    "            for i, orig_word in enumerate(original_words):\n",
    "                if i < len(standardized_words):\n",
    "                    if standardized_words[i].startswith(orig_word[:3]):  # First 3 chars match\n",
    "                        initial_match_score += 1.0\n",
    "            initial_match_ratio = initial_match_score / len(original_words) if original_words else 0\n",
    "            \n",
    "            # High confidence if most words follow abbreviation pattern\n",
    "            if initial_match_ratio >= 0.7:\n",
    "                return 0.95  # Very high confidence for good abbreviations\n",
    "            elif initial_match_ratio >= 0.5:\n",
    "                return 0.85\n",
    "            else:\n",
    "                return 0.75\n",
    "        \n",
    "        # Default confidence for other abbreviation patterns\n",
    "        return 0.8\n",
    "\n",
    "    def _assess_spelling_correction_quality(self, original: str, standardized: str) -> float:\n",
    "        \"\"\"\n",
    "        Assess the quality of spelling correction\n",
    "        Based on edit distance and phonetic similarity\n",
    "        \"\"\"\n",
    "        from rapidfuzz.distance import Levenshtein\n",
    "        edit_distance = Levenshtein.distance(original, standardized)            \n",
    "        max_len = max(len(original), len(standardized))\n",
    "        \n",
    "        if max_len == 0:\n",
    "            return 0.5\n",
    "        \n",
    "        # Confidence decreases with edit distance\n",
    "        edit_ratio = 1 - (edit_distance / max_len)\n",
    "        \n",
    "        # High confidence for minor spelling errors\n",
    "        if edit_distance <= 2:\n",
    "            return 0.9\n",
    "        elif edit_distance <= 4:\n",
    "            return 0.8\n",
    "        else:\n",
    "            return max(0.6, edit_ratio)\n",
    "   \n",
    "    def standardize_occupation(self, occupation_str: str) -> Dict:\n",
    "        \"\"\"\n",
    "        ENHANCED: Standardize occupation string with multi-occupation detection\n",
    "        \"\"\"\n",
    "        if pd.isna(occupation_str):\n",
    "            return {\n",
    "                'original': '',\n",
    "                'standardized': '',\n",
    "                'category': 'unknown',\n",
    "                'confidence': 0.0,\n",
    "                'needs_llm_check': True,\n",
    "                'confidence_reason': 'empty_input',\n",
    "                'invalid_words': [],\n",
    "                'is_multiple_occupation': False  # NEW FIELD: Simple boolean flag\n",
    "            }\n",
    "        \n",
    "        original = str(occupation_str).strip()\n",
    "        \n",
    "        # NEW: Check for multiple occupations (simple detection)\n",
    "        is_multiple_occupation = self._detect_multiple_occupations(original)\n",
    "        \n",
    "        # PHASE 1: Preprocessing - Fix modifier variants and multi-word spellings\n",
    "        # Step 1: Normalize the entire string\n",
    "        normalized_full = self.normalize_text(original)\n",
    "        \n",
    "        # Step 2: First spelling correction - Handle modifier variants (daur→daughter)\n",
    "        # This step will convert \"black smith daur\" → \"black smith daughter\"\n",
    "        first_correction = self.correct_spelling(normalized_full)\n",
    "        is_misspelled_phase1 = first_correction != normalized_full\n",
    "        \n",
    "        # PHASE 2: Modifier detection and base occupation processing  \n",
    "        # Step 3: Detect modifiers on the corrected text\n",
    "        modifiers, base_text = self.detect_modifiers(first_correction)\n",
    "        \n",
    "        # Step 4: Normalize base occupation text\n",
    "        normalized_base = self.normalize_text(base_text)\n",
    "        \n",
    "        # Step 5: Expand abbreviations\n",
    "        expanded = self.expand_abbreviations(normalized_base)\n",
    "        is_abbreviation = expanded != normalized_base\n",
    "        \n",
    "        # Step 6: Second spelling correction - Handle base occupation spelling (black smith→blacksmith)\n",
    "        corrected_base = self.correct_spelling(expanded)\n",
    "        is_misspelled_phase2 = corrected_base != expanded\n",
    "        \n",
    "        # Combine spelling error detection from both phases\n",
    "        is_misspelled = is_misspelled_phase1 or is_misspelled_phase2\n",
    "        \n",
    "        # Step 7: Fuzzy match to standard occupations\n",
    "        matched_base = self.fuzzy_match_occupations(corrected_base, threshold=75)\n",
    "        \n",
    "        # Step 8: Recombine standardized result\n",
    "        standardized = self.standardize_with_modifiers(original, modifiers, matched_base)\n",
    "        \n",
    "        # Step 9: Determine occupation category\n",
    "        category = self.get_occupation_category_with_modifiers(standardized, modifiers)\n",
    "        \n",
    "        # Step 10: Calculate confidence score (includes dictionary validation)\n",
    "        confidence = self.calculate_confidence(original, standardized, is_abbreviation, is_misspelled)\n",
    "\n",
    "        # NEW: Adjust confidence if multiple occupation detected\n",
    "        if is_multiple_occupation:\n",
    "            confidence = confidence * 0.9  # Small penalty for multi-occupation complexity\n",
    "\n",
    "        # Determine if LLM check is needed (considering multiple occupation)\n",
    "        needs_llm_check, confidence_reason, invalid_words = self._determine_llm_check_need(\n",
    "            original, standardized, confidence, is_abbreviation, is_misspelled, is_multiple_occupation\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'original': original,\n",
    "            'standardized': standardized,  \n",
    "            'category': category,\n",
    "            'confidence': confidence,\n",
    "            'needs_llm_check': needs_llm_check,\n",
    "            'confidence_reason': confidence_reason,\n",
    "            'invalid_words': invalid_words,\n",
    "            'is_multiple_occupation': is_multiple_occupation  # NEW FIELD: Simple boolean flag\n",
    "        }\n",
    "    \n",
    "    def _determine_llm_check_need(self, original: str, standardized: str, confidence: float,\n",
    "                                is_abbreviation: bool, is_misspelled: bool, is_multiple_occupation: bool = False) -> Tuple[bool, str, List[str]]:\n",
    "        \"\"\"\n",
    "        ENHANCED: Determine if LLM check is needed (now considers multiple occupation)\n",
    "        \"\"\"\n",
    "        original_clean = str(original).lower().strip()\n",
    "        standardized_clean = str(standardized).lower().strip()\n",
    "        is_unchanged = original_clean == standardized_clean\n",
    "        \n",
    "        # Validate original text for invalid words\n",
    "        original_validation = self._validate_english_words(original_clean)\n",
    "        \n",
    "        # Priority 1: Multiple occupation - EXCLUDE from LLM check\n",
    "        if is_multiple_occupation:\n",
    "            return False, 'multiple_occupation_excluded_from_llm_check', original_validation['invalid_words']\n",
    "        \n",
    "        # Priority 2: Unchanged with invalid words (highest priority)\n",
    "        if is_unchanged and original_validation['contains_invalid']:\n",
    "            if original_validation['validity_ratio'] < 0.5:\n",
    "                return True, 'unchanged_with_many_invalid_words', original_validation['invalid_words']\n",
    "            elif original_validation['validity_ratio'] < 0.8:\n",
    "                return True, 'unchanged_with_some_invalid_words', original_validation['invalid_words']\n",
    "            else:\n",
    "                return False, 'unchanged_with_few_invalid_words', original_validation['invalid_words']\n",
    "        \n",
    "        # Priority 3: Very low confidence regardless of change\n",
    "        if confidence < 0.3:\n",
    "            return True, 'very_low_confidence', original_validation['invalid_words']\n",
    "        \n",
    "        # Priority 4: Modified but still has quality issues\n",
    "        if not is_unchanged:\n",
    "            standardized_validation = self._validate_english_words(standardized_clean)\n",
    "            if standardized_validation['validity_ratio'] < 0.7:\n",
    "                return True, 'modification_poor_result', standardized_validation['invalid_words']\n",
    "        \n",
    "        # Priority 5: Low confidence\n",
    "        if confidence < 0.5:\n",
    "            return True, 'low_confidence', original_validation['invalid_words']\n",
    "        \n",
    "        # No LLM check needed\n",
    "        return False, 'acceptable_quality', []\n",
    "    \n",
    "    def detect_modifiers(self, text: str) -> Tuple[List[str], str]:\n",
    "        \"\"\"\n",
    "        Detect important modifier words in occupation text\n",
    "        Returns: (list_of_modifiers, base_occupation_text)\n",
    "        \"\"\"\n",
    "        if pd.isna(text):\n",
    "            return [], \"\"\n",
    "        \n",
    "        normalized_text = self.normalize_text(text)\n",
    "        words = normalized_text.split()\n",
    "        \n",
    "        detected_modifiers = []\n",
    "        remaining_words = []\n",
    "        \n",
    "        modifier_set = getattr(self, 'all_modifiers_set', set())\n",
    "        if not modifier_set and self.important_modifiers:\n",
    "            for modifier_list in self.important_modifiers.values():\n",
    "                if isinstance(modifier_list, list):\n",
    "                    modifier_set.update(modifier_list)\n",
    "        \n",
    "        for word in words:\n",
    "            if word in modifier_set:\n",
    "                detected_modifiers.append(word)\n",
    "            else:\n",
    "                remaining_words.append(word)\n",
    "        \n",
    "        base_text = \" \".join(remaining_words).strip()\n",
    "        return detected_modifiers, base_text\n",
    "\n",
    "    def standardize_with_modifiers(self, text: str, modifiers: List[str], base_occupation: str) -> str:\n",
    "        \"\"\"\n",
    "        Standardize occupation while preserving important modifiers\n",
    "        \"\"\"\n",
    "        if not base_occupation:\n",
    "            return text\n",
    "        \n",
    "        standardized_base = self.fuzzy_match_occupations(base_occupation, threshold=75)\n",
    "        \n",
    "        if not modifiers:\n",
    "            return standardized_base\n",
    "        \n",
    "        family_words = self.important_modifiers.get('family_relation', [])\n",
    "        family_modifiers = [m for m in modifiers if m in family_words]\n",
    "        \n",
    "        if family_modifiers:\n",
    "            if 'wife' in family_modifiers:\n",
    "                return f\"{standardized_base} wife\"\n",
    "            elif 'widow' in family_modifiers:\n",
    "                return f\"{standardized_base} widow\" \n",
    "            elif 'daughter' in family_modifiers:\n",
    "                return f\"{standardized_base} daughter\"\n",
    "            elif 'son' in family_modifiers:\n",
    "                return f\"{standardized_base} son\"\n",
    "            else:\n",
    "                return f\"{standardized_base} {family_modifiers[0]}\"\n",
    "        \n",
    "        skill_words = self.important_modifiers.get('skill_level', [])\n",
    "        skill_modifiers = [m for m in modifiers if m in skill_words]\n",
    "        if skill_modifiers:\n",
    "            return f\"{skill_modifiers[0]} {standardized_base}\"\n",
    "        \n",
    "        employment_words = self.important_modifiers.get('employment_status', [])\n",
    "        employment_modifiers = [m for m in modifiers if m in employment_words]\n",
    "        if employment_modifiers:\n",
    "            return f\"{employment_modifiers[0]} {standardized_base}\"\n",
    "        \n",
    "        if modifiers:\n",
    "            return f\"{' '.join(modifiers)} {standardized_base}\".strip()\n",
    "        \n",
    "        return standardized_base\n",
    "\n",
    "    def get_occupation_category_with_modifiers(self, occupation: str, modifiers: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Determine category considering modifiers\n",
    "        \"\"\"\n",
    "        if not modifiers:\n",
    "            return self.get_occupation_category(occupation)\n",
    "        \n",
    "        # Check for family-related modifiers\n",
    "        family_words = self.important_modifiers.get('family_relation', [])\n",
    "        if any(m in family_words for m in modifiers):\n",
    "            return 'family_dependent'\n",
    "        \n",
    "        # Check for unemployment\n",
    "        employment_words = self.important_modifiers.get('employment_status', [])\n",
    "        if any(m in employment_words for m in modifiers):\n",
    "            return 'unemployed_seeking'\n",
    "        \n",
    "        # Otherwise use standard category detection\n",
    "        return self.get_occupation_category(occupation)\n",
    "    \n",
    "    def batch_standardize_occupations(self, df: pd.DataFrame, chunk_size: int = 1000) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        ENHANCED: Standardize all occupations with simple multi-occupation flagging\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        unique_occupations = df[\"Occupation_String\"].unique()\n",
    "        \n",
    "        print(f\"Standardizing {len(unique_occupations)} unique occupations...\")\n",
    "        print(\"Multi-occupation detection enabled (simple flagging)\")\n",
    "        \n",
    "        # Track multi-occupation statistics\n",
    "        multi_occupation_count = 0\n",
    "        \n",
    "        for i in tqdm(range(0, len(unique_occupations), chunk_size), desc=\"Processing occupation chunks\"):\n",
    "            chunk_occupations = unique_occupations[i:i + chunk_size]\n",
    "            chunk_results = []\n",
    "            \n",
    "            for occupation in chunk_occupations:\n",
    "                result = self.standardize_occupation(occupation)\n",
    "                \n",
    "                # Track multi-occupation statistics\n",
    "                if result.get('is_multiple_occupation', False):\n",
    "                    multi_occupation_count += 1\n",
    "                \n",
    "                chunk_results.append(result)\n",
    "            \n",
    "            results.extend(chunk_results)\n",
    "            gc.collect()\n",
    "        \n",
    "        standardization_df = pd.DataFrame(results)\n",
    "        \n",
    "        print(\"Merging results with original DataFrame by county...\")\n",
    "        df_chunks = []\n",
    "        \n",
    "        for county in tqdm(df['County'].unique(), desc=\"Processing counties\"):\n",
    "            county_df = df[df['County'] == county].copy()\n",
    "            \n",
    "            for i in range(0, len(county_df), chunk_size):\n",
    "                chunk_df = county_df.iloc[i:i + chunk_size].copy()\n",
    "                chunk_merged = chunk_df.merge(\n",
    "                    standardization_df, \n",
    "                    left_on=\"Occupation_String\", \n",
    "                    right_on=\"original\", \n",
    "                    how=\"left\"\n",
    "                )\n",
    "                df_chunks.append(chunk_merged)\n",
    "        \n",
    "        df_standardized = pd.concat(df_chunks, ignore_index=True)\n",
    "        df_standardized = df_standardized.drop(columns=['original'])\n",
    "\n",
    "        # Print comprehensive summary statistics\n",
    "        total_records = len(df_standardized)\n",
    "        llm_needed = df_standardized['needs_llm_check'].sum()\n",
    "        llm_percentage = (llm_needed / total_records) * 100 if total_records > 0 else 0\n",
    "        avg_confidence = df_standardized['confidence'].mean()\n",
    "        \n",
    "        # Multi-occupation statistics\n",
    "        multi_occupation_records = df_standardized['is_multiple_occupation'].sum()\n",
    "        multi_occupation_percentage = (multi_occupation_records / total_records) * 100 if total_records > 0 else 0\n",
    "        \n",
    "        print(f\"\\n=== ENHANCED PROCESSING SUMMARY ===\")\n",
    "        print(f\"Total records processed: {total_records:,}\")\n",
    "        print(f\"Records needing LLM check: {llm_needed:,} ({llm_percentage:.1f}%)\")\n",
    "        print(f\"Average confidence score: {avg_confidence:.3f}\")\n",
    "        print(f\"Multi-occupation records detected: {multi_occupation_records:,} ({multi_occupation_percentage:.1f}%)\")\n",
    "        \n",
    "        # Show top reasons for LLM check\n",
    "        if llm_needed > 0:\n",
    "            print(f\"\\nTop reasons for LLM check:\")\n",
    "            reason_counts = df_standardized['confidence_reason'].value_counts()\n",
    "            for reason, count in reason_counts.head(7).items():\n",
    "                percentage = (count / total_records) * 100\n",
    "                print(f\"  {reason}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "        return df_standardized, standardization_df\n",
    "\n",
    "def Occupation_cleaning_pipeline(df: pd.DataFrame, \n",
    "                                excel_file_path: str = None,\n",
    "                                min_frequency: int = 2,\n",
    "                                chunk_size: int = 1000):\n",
    "    \"\"\"\n",
    "    Complete traditional cleaning pipeline with simple multi-occupation detection\n",
    "    \"\"\"\n",
    "    cleaner = TraditionalOccupationCleaner(excel_file_path)\n",
    "    \n",
    "    print(f\"Processing {len(df)} records in chunks of {chunk_size}...\")\n",
    "    print(\"Multi-occupation detection: Simple flagging enabled\")\n",
    "    \n",
    "    # Standardize occupations\n",
    "    print(\"Standardizing occupations...\")\n",
    "    df_standardized, standardization_results = cleaner.batch_standardize_occupations(df, chunk_size)\n",
    "\n",
    "    return df_standardized   \n",
    "\n",
    "def integrate_traditional_cleaning_with_export(df: pd.DataFrame, \n",
    "                                             excel_file_path: str = None,\n",
    "                                             chunk_size: int = 1000,\n",
    "                                             export_mapping: bool = True,\n",
    "                                             output_dir: str = \".\"):\n",
    "    \"\"\"\n",
    "    ENHANCED: Memory-efficient integration with simple multi-occupation detection and LLM flagging\n",
    "    \"\"\"\n",
    "    n_records = len(df)\n",
    "    n_unique_occupations = len(df[\"Occupation_String\"].unique())\n",
    "    \n",
    "    print(f\"Dataset: {n_records} records, {n_unique_occupations} unique occupations\")\n",
    "    print(f\"Counties: {len(df['County'].unique())}\")\n",
    "\n",
    "    df_cleaned = Occupation_cleaning_pipeline(\n",
    "        df, \n",
    "        excel_file_path,\n",
    "        chunk_size=chunk_size\n",
    "    ) \n",
    "    df_result = df_cleaned\n",
    "    \n",
    "    main_output = os.path.join(output_dir, \"cleaned_occupations_with_categories.csv\")\n",
    "    df_result.to_csv(main_output, index=False)\n",
    "    print(f\"Exported main dataset to {main_output}\")\n",
    "\n",
    "    # Export LLM check subset\n",
    "    llm_check_subset = df_result[df_result['needs_llm_check'] == True].copy()\n",
    "    if len(llm_check_subset) > 0:\n",
    "        # Sort by confidence (lowest first) for prioritized review\n",
    "        llm_check_subset = llm_check_subset.sort_values('confidence')\n",
    "        llm_output = os.path.join(output_dir, \"llm_check_needed.csv\")\n",
    "        llm_check_subset.to_csv(llm_output, index=False)\n",
    "        print(f\"Exported LLM check subset to {llm_output} ({len(llm_check_subset):,} records)\")\n",
    "\n",
    "    # NEW: Export multi-occupation flagged records\n",
    "    multi_occupation_subset = df_result[df_result['is_multiple_occupation'] == True].copy()\n",
    "    if len(multi_occupation_subset) > 0:\n",
    "        multi_output = os.path.join(output_dir, \"multi_occupation_flagged.csv\")\n",
    "        multi_occupation_subset.to_csv(multi_output, index=False)\n",
    "        print(f\"Exported multi-occupation flagged records to {multi_output} ({len(multi_occupation_subset):,} records)\")\n",
    "\n",
    "    return df_result\n",
    "\n",
    "\n",
    "# Usage example with simple multi-occupation detection\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your data\n",
    "    df = sample\n",
    "    \n",
    "    # Ensure County column exists\n",
    "    if 'County' not in df.columns:\n",
    "        print(\"Warning: 'County' column not found. Adding default county...\")\n",
    "        df['County'] = 'Unknown'\n",
    "\n",
    "    # Set parameters\n",
    "    CHUNK_SIZE = 1000\n",
    "    EXCEL_FILE_PATH = \"occupation_dictionaries.xlsx\"\n",
    "    \n",
    "    print(f\"Loaded {len(df)} records from {len(df['County'].unique())} counties.\")\n",
    "    print(f\"Processing in chunks of {CHUNK_SIZE} per county...\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(\"./cleaning_results_1851_validation\", exist_ok=True)\n",
    "    \n",
    "    # Run enhanced cleaning pipeline with simple multi-occupation detection\n",
    "    df_cleaned = integrate_traditional_cleaning_with_export(\n",
    "        df, \n",
    "        excel_file_path=EXCEL_FILE_PATH,\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        export_mapping=True,\n",
    "        output_dir=\"./cleaning_results_1851_validation\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nProcessing completed! Cleaned {len(df_cleaned)} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Process with LLM:Qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 19:57:55,081 - INFO -  Starting optimized single record Ollama processing with unique deduplication\n",
      "2025-08-20 19:57:55,083 - INFO -  Input data: 189 records\n",
      "2025-08-20 19:57:55,083 - INFO -  Single record mode with 6 workers\n",
      "2025-08-20 19:57:55,083 - INFO -  Threading: Enabled\n",
      "2025-08-20 19:57:55,084 - INFO -  Unique processing: Enabled\n",
      "2025-08-20 19:57:55,084 - INFO -  Available auxiliary columns: ['County', 'Sex', 'Occode_Desc']\n",
      "2025-08-20 19:57:55,085 - INFO - 🔍 Deduplication analysis:\n",
      "2025-08-20 19:57:55,086 - INFO -   Total records to process: 189\n",
      "2025-08-20 19:57:55,086 - INFO -   Unique occupation strings: 187\n",
      "2025-08-20 19:57:55,087 - INFO -   Potential processing savings: 2 (1.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset overview:\n",
      "Total records: 189\n",
      "Low confidence records: 189\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-20 19:57:59,191 - ERROR -  Cannot connect to Ollama service: HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/tags (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024A8D095350>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "2025-08-20 19:57:59,192 - INFO -  Optimized single processor initialized\n",
      "2025-08-20 19:57:59,192 - INFO -  Max workers: 6, Delay: 0.1s\n",
      "2025-08-20 19:57:59,193 - INFO -  Starting optimized single record processing with unique deduplication\n",
      "2025-08-20 19:57:59,193 - INFO -  Total records: 189\n",
      "2025-08-20 19:57:59,194 - INFO -  Threading: Enabled (6 workers)\n",
      "2025-08-20 19:57:59,194 - INFO -  Unique processing: Enabled\n",
      "2025-08-20 19:57:59,196 - INFO -  Records below confidence threshold: 189\n",
      "2025-08-20 19:57:59,234 - INFO -  Unique occupation strings to process: 187\n",
      "2025-08-20 19:57:59,234 - INFO -  Duplicate records saved from processing: 2\n",
      "2025-08-20 19:57:59,235 - INFO -  Deduplication efficiency: 1.1%\n",
      "2025-08-20 19:57:59,236 - INFO -  Most frequent occupation strings:\n",
      "2025-08-20 19:57:59,236 - INFO -   'farm lab': 2 occurrences\n",
      "2025-08-20 19:57:59,238 - INFO -   'bailiff on a farm': 2 occurrences\n",
      "2025-08-20 19:57:59,242 - INFO -  Records to process: 187\n",
      "2025-08-20 19:57:59,243 - INFO -  Already processed: 0\n",
      "Processing records:   0%|          | 0/187 [00:00<?, ?it/s]2025-08-20 19:58:03,316 - ERROR - API request failed (attempt 1): HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024AA4FEA7D0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "2025-08-20 19:58:03,331 - ERROR - API request failed (attempt 1): HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024AACABD390>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "2025-08-20 19:58:03,333 - ERROR - API request failed (attempt 1): HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024AAFE51410>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "2025-08-20 19:58:03,334 - ERROR - API request failed (attempt 1): HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024B128A8790>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "2025-08-20 19:58:03,334 - ERROR - API request failed (attempt 1): HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024AAFE56C50>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "2025-08-20 19:58:03,334 - ERROR - API request failed (attempt 1): HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024AA4C80350>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "2025-08-20 19:58:07,861 - ERROR - API request failed (attempt 2): HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024B125A1D90>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "2025-08-20 19:58:07,877 - ERROR - API request failed (attempt 2): HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024B129D9090>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "2025-08-20 19:58:07,878 - ERROR - API request failed (attempt 2): HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024B129E9A10>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "2025-08-20 19:58:07,879 - ERROR - API request failed (attempt 2): HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024ADD811AD0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "2025-08-20 19:58:07,880 - ERROR - API request failed (attempt 2): HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024B12BABC90>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "2025-08-20 19:58:07,892 - ERROR - API request failed (attempt 2): HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024AE46AF750>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "Processing records:   3%|▎         | 6/187 [00:09<01:47,  1.68it/s]2025-08-20 19:58:11,944 - ERROR - API request failed (attempt 1): HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024A8D01A1D0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "2025-08-20 19:58:11,958 - ERROR - API request failed (attempt 1): HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024B134CBED0>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "2025-08-20 19:58:11,958 - ERROR - API request failed (attempt 1): HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024B134CBF90>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "2025-08-20 19:58:11,974 - ERROR - API request failed (attempt 1): HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024B12BDB010>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "2025-08-20 19:58:11,988 - ERROR - API request failed (attempt 1): HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024AE46D3110>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "2025-08-20 19:58:11,988 - ERROR - API request failed (attempt 1): HTTPConnectionPool(host='localhost', port=11434): Max retries exceeded with url: /api/generate (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x0000024B135A0A10>: Failed to establish a new connection: [WinError 10061] 由于目标计算机积极拒绝，无法连接。'))\n",
      "2025-08-20 19:58:35,275 - ERROR - API request failed (attempt 2): 500 Server Error: Internal Server Error for url: http://localhost:11434/api/generate\n",
      "Processing records: 100%|██████████| 187/187 [05:34<00:00,  1.79s/it]\n",
      "2025-08-20 20:03:34,166 - INFO -  Checkpoint saved: 187 records processed\n",
      "2025-08-20 20:03:34,169 - INFO -  Mapping unique results back to original dataset...\n",
      "2025-08-20 20:03:34,225 - INFO -  Applied results to 189 records\n",
      "2025-08-20 20:03:34,226 - INFO -  Optimized unique record processing completed!\n",
      "2025-08-20 20:03:34,229 - INFO - \n",
      "============================================================\n",
      "2025-08-20 20:03:34,229 - INFO -  OPTIMIZED SINGLE PROCESSING STATISTICS\n",
      "2025-08-20 20:03:34,230 - INFO - ============================================================\n",
      "2025-08-20 20:03:34,230 - INFO - Total records processed: 187\n",
      "2025-08-20 20:03:34,230 - INFO - Total corrections made: 95\n",
      "2025-08-20 20:03:34,231 - INFO - API errors: 0\n",
      "2025-08-20 20:03:34,231 - INFO - Unique records processed: 187\n",
      "2025-08-20 20:03:34,231 - INFO - Duplicate records saved: 2\n",
      "2025-08-20 20:03:34,232 - INFO - Deduplication efficiency: 1.1%\n",
      "2025-08-20 20:03:34,232 - INFO - Success rate: 100.0%\n",
      "2025-08-20 20:03:34,232 - INFO - Correction rate: 50.8%\n",
      "2025-08-20 20:03:34,233 - INFO - Average processing speed: 0.1 records/second\n",
      "2025-08-20 20:03:34,233 - INFO - Average response time: 10.60 seconds\n",
      "2025-08-20 20:03:34,233 - INFO - Total processing time: 0.55 hours\n",
      "2025-08-20 20:03:34,234 - INFO - ============================================================\n",
      "2025-08-20 20:03:34,261 - INFO -  Final results saved to: .\\cleaning_results_1851_validation\\df_llm_check_single_1911.csv\n",
      "2025-08-20 20:03:34,271 - INFO - \n",
      " Correction examples (total: 90):\n",
      "2025-08-20 20:03:34,272 - INFO -   'deastrish' → 'deastrish wife'\n",
      "2025-08-20 20:03:34,272 - INFO -     (Original: deastrish)\n",
      "2025-08-20 20:03:34,274 - INFO -   'hea kinter' → 'hospital kinter'\n",
      "2025-08-20 20:03:34,275 - INFO -     (Original: hea kinter)\n",
      "2025-08-20 20:03:34,275 - INFO -   'storekeeper h m docky yd' → 'storekeeper h m dockyard'\n",
      "2025-08-20 20:03:34,275 - INFO -     (Original: storekeeper h m docky yd)\n",
      "2025-08-20 20:03:34,276 - INFO -   'm r c s l s a in general practice' → 'medical surgeon'\n",
      "2025-08-20 20:03:34,277 - INFO -     (Original: m r c s l s a in general practice)\n",
      "2025-08-20 20:03:34,277 - INFO -   'capens steward r n' → 'capen steward r n'\n",
      "2025-08-20 20:03:34,277 - INFO -     (Original: capens steward r n)\n",
      "2025-08-20 20:03:34,278 - INFO - \n",
      " Unique Processing Efficiency:\n",
      "2025-08-20 20:03:34,279 - INFO -   Would have processed: 189 records\n",
      "2025-08-20 20:03:34,280 - INFO -   Actually processed: 187 unique records\n",
      "2025-08-20 20:03:34,280 - INFO -   Processing saved: 2 records (1.1%)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total corrections made: 90\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "import string\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class OptimizedSingleProcessor:\n",
    "    \"\"\"\n",
    "    Optimized single record processor - accurate and efficient\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name: str = \"llama2:7b\",\n",
    "                 ollama_url: str = \"http://localhost:11434\",\n",
    "                 timeout: int = 45,\n",
    "                 max_retries: int = 2,\n",
    "                 delay_between_requests: float = 0.1,  # Very fast processing\n",
    "                 max_workers: int = 6):                # More workers for single requests\n",
    "        \"\"\"\n",
    "        Initialize optimized single processor\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.ollama_url = ollama_url\n",
    "        self.timeout = timeout\n",
    "        self.max_retries = max_retries\n",
    "        self.delay_between_requests = delay_between_requests\n",
    "        self.max_workers = max_workers\n",
    "        \n",
    "        # Thread lock for statistics\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        # Statistics\n",
    "        self.stats = {\n",
    "            \"total_processed\": 0,\n",
    "            \"corrections_made\": 0,\n",
    "            \"api_errors\": 0,\n",
    "            \"processing_time\": 0,\n",
    "            \"average_response_time\": 0,\n",
    "            \"unique_records_processed\": 0,        # NEW: Track unique records\n",
    "            \"duplicate_records_saved\": 0,         # NEW: Track duplicates saved\n",
    "            \"deduplication_ratio\": 0.0            # NEW: Track deduplication efficiency\n",
    "        }\n",
    "        \n",
    "        # Test connection\n",
    "        self._test_connection()\n",
    "        logger.info(f\" Optimized single processor initialized\")\n",
    "        logger.info(f\" Max workers: {max_workers}, Delay: {delay_between_requests}s\")\n",
    "    \n",
    "    def _test_connection(self):\n",
    "        \"\"\"Test Ollama connection\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.ollama_url}/api/tags\", timeout=10)\n",
    "            if response.status_code == 200:\n",
    "                models = response.json().get(\"models\", [])\n",
    "                model_names = [m[\"name\"] for m in models]\n",
    "                logger.info(f\" Ollama connection successful! Available models: {model_names}\")\n",
    "                \n",
    "                if self.model_name not in model_names:\n",
    "                    logger.warning(f\"  Model '{self.model_name}' not found. Please run: ollama pull {self.model_name}\")\n",
    "            else:\n",
    "                logger.error(f\" Ollama connection failed: HTTP {response.status_code}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\" Cannot connect to Ollama service: {e}\")\n",
    "    \n",
    "    def create_optimized_single_prompt(self, original: str, rule_based_result: str, \n",
    "                                     confidence: float, county: str = \"\", sex: str = \"\", Occode_Desc: str = \"\") -> str:\n",
    "        \"\"\"\n",
    "        Create optimized prompt for single record processing\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add context if available\n",
    "        context = \"\"\n",
    "        if county and county != \"Unknown\":\n",
    "            context += f\" County: {county}.\"\n",
    "        if sex and sex != \"Unknown\":\n",
    "            context += f\" Gender: {sex}.\"\n",
    "        if Occode_Desc and Occode_Desc != \"Unknown\":\n",
    "            context += f\" Description for Original:{Occode_Desc}.\"\n",
    "        \n",
    "        prompt = f\"\"\"Fix the spelling and formatting of this British historical occupation. Keep the meaning exactly the same.\n",
    "\n",
    "Original: \"{original}\"\n",
    "Current: \"{rule_based_result}\"\n",
    "Confidence: {confidence:.2f}{context}\n",
    "\n",
    "Rules:\n",
    "- ONLY fix spelling mistakes and formatting\n",
    "- Keep family terms: wife, widow, daughter, son\n",
    "- Keep core occupation unchanged (blacksmith stays blacksmith)\n",
    "- If Original is good, keep it\n",
    "- Do NOT change occupation meaning\n",
    "\n",
    "Examples:\n",
    "- \"black smith\" → \"blacksmith\" (fix spacing)\n",
    "- \"ag lab\" → \"agricultural labourer\" (standard abbreviation)\n",
    "- \"black smith wife\" → \"blacksmith wife\" (fix spacing, keep family term)\n",
    "\n",
    "Output only the corrected occupation:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def call_ollama_optimized(self, prompt: str) -> str:\n",
    "        \"\"\"\n",
    "        Optimized Ollama API call for single records\n",
    "        \"\"\"\n",
    "        for attempt in range(self.max_retries):\n",
    "            try:\n",
    "                response = requests.post(\n",
    "                    f\"{self.ollama_url}/api/generate\",\n",
    "                    json={\n",
    "                        \"model\": self.model_name,\n",
    "                        \"prompt\": prompt,\n",
    "                        \"stream\": False,\n",
    "                        \"options\": {\n",
    "                            \"temperature\": 0.0,         # Zero randomness for consistency\n",
    "                            \"num_predict\": 50,          # Short responses for single occupations\n",
    "                            \"top_k\": 1,                 # Most deterministic\n",
    "                            \"top_p\": 0.1,               # Very focused\n",
    "                            \"repeat_penalty\": 1.0,     # No penalty needed\n",
    "                            \"stop\": [\"Original:\", \"Current:\", \"Rules:\", \"Examples:\", \"Output\"]\n",
    "                        }\n",
    "                    },\n",
    "                    timeout=self.timeout\n",
    "                )\n",
    "                response.raise_for_status()\n",
    "                \n",
    "                result = response.json()\n",
    "                return result.get(\"response\", \"\").strip()\n",
    "                \n",
    "            except requests.exceptions.Timeout:\n",
    "                logger.warning(f\"API timeout (attempt {attempt + 1}/{self.max_retries})\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(1)  # Short wait before retry\n",
    "                else:\n",
    "                    return \"\"\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"API request failed (attempt {attempt + 1}): {e}\")\n",
    "                if attempt < self.max_retries - 1:\n",
    "                    time.sleep(0.5)\n",
    "                else:\n",
    "                    return \"\"\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "    def parse_single_response_clean(self, response_text: str, original_result: str) -> str:\n",
    "        \"\"\"\n",
    "        Clean parser for single record responses\n",
    "        \"\"\"\n",
    "        if not response_text:\n",
    "            return original_result\n",
    "        \n",
    "        # Clean the response\n",
    "        cleaned = response_text.strip()\n",
    "        \n",
    "        # Remove common prefixes that LLM might add\n",
    "        prefixes = [\n",
    "            \"Output:\", \"Result:\", \"Corrected:\", \"Answer:\", \"The corrected occupation is:\",\n",
    "            \"Corrected occupation:\", \"Fixed:\", \"Final result:\"\n",
    "        ]\n",
    "        \n",
    "        for prefix in prefixes:\n",
    "            if cleaned.lower().startswith(prefix.lower()):\n",
    "                cleaned = cleaned[len(prefix):].strip()\n",
    "                break\n",
    "        \n",
    "        # Remove quotes and punctuation\n",
    "        cleaned = cleaned.strip('\"\\'.,!?')\n",
    "        \n",
    "        # Remove any trailing explanation\n",
    "        # Split on common explanation starters\n",
    "        explanation_starters = [\" because\", \" since\", \" as\", \" (\", \" -\", \"\\n\"]\n",
    "        for starter in explanation_starters:\n",
    "            if starter in cleaned:\n",
    "                cleaned = cleaned.split(starter)[0].strip()\n",
    "        \n",
    "        # Validate result\n",
    "        if not cleaned or len(cleaned) > 100:\n",
    "            return original_result\n",
    "        \n",
    "        # Check for error responses\n",
    "        error_words = ['sorry', 'cannot', 'unclear', 'error', 'invalid', 'unsure', 'difficult']\n",
    "        if any(word in cleaned.lower() for word in error_words):\n",
    "            return original_result\n",
    "        \n",
    "        # Additional validation - check if result is reasonable\n",
    "        if self._validate_single_result(cleaned, original_result):\n",
    "            return cleaned\n",
    "        else:\n",
    "            return original_result\n",
    "    \n",
    "    def _validate_single_result(self, llm_result: str, original_result: str) -> bool:\n",
    "        \"\"\"\n",
    "        Validate that LLM result is reasonable for single record\n",
    "        \"\"\"\n",
    "        llm_lower = llm_result.lower()\n",
    "        orig_lower = original_result.lower()\n",
    "        \n",
    "        # If same as original, it's valid\n",
    "        if llm_lower == orig_lower:\n",
    "            return True\n",
    "        \n",
    "        # Check length - shouldn't be too different\n",
    "        if abs(len(llm_result) - len(original_result)) > 20:\n",
    "            return False\n",
    "        \n",
    "        # Should contain similar key words\n",
    "        llm_words = set(llm_lower.split())\n",
    "        orig_words = set(orig_lower.split())\n",
    "        \n",
    "        # At least 50% word overlap for short occupations\n",
    "        if len(orig_words) <= 3:\n",
    "            overlap = len(llm_words & orig_words) / len(orig_words | llm_words)\n",
    "            if overlap < 0.3:\n",
    "                return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def process_single_record_optimized(self, record: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Process single record with optimization\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # Create prompt\n",
    "            prompt = self.create_optimized_single_prompt(\n",
    "                record['original'],\n",
    "                record['rule_based_result'],\n",
    "                record['confidence'],\n",
    "                record.get('county', ''),\n",
    "                record.get('sex', ''),\n",
    "                record.get('Occode_Desc', '')\n",
    "            )\n",
    "            \n",
    "            # Call LLM\n",
    "            llm_response = self.call_ollama_optimized(prompt)\n",
    "            \n",
    "            # Parse result\n",
    "            final_result = self.parse_single_response_clean(llm_response, record['rule_based_result'])\n",
    "            \n",
    "            # Determine if correction was made\n",
    "            is_corrected = final_result != record['rule_based_result']\n",
    "            \n",
    "            processing_time = time.time() - start_time\n",
    "            \n",
    "            # Update statistics\n",
    "            with self.lock:\n",
    "                self.stats[\"total_processed\"] += 1\n",
    "                if is_corrected:\n",
    "                    self.stats[\"corrections_made\"] += 1\n",
    "                self.stats[\"processing_time\"] += processing_time\n",
    "                \n",
    "                # Update average response time\n",
    "                self.stats[\"average_response_time\"] = self.stats[\"processing_time\"] / self.stats[\"total_processed\"]\n",
    "            \n",
    "            return {\n",
    "                'Occupation_String': record['original'],\n",
    "                'rule_based_result': record['rule_based_result'],\n",
    "                'rule_based_confidence': record['confidence'],\n",
    "                'county': record.get('county', 'Unknown'),\n",
    "                'sex': record.get('sex', 'Unknown'),\n",
    "                'Occode_Desc': record.get('Occode_Desc', 'Unknown'),\n",
    "                'llm_corrected': final_result,\n",
    "                'is_corrected': is_corrected,\n",
    "                'final_result': final_result,\n",
    "                'processing_time': processing_time,\n",
    "                'llm_response_raw': llm_response[:100] if llm_response else ''\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing record: {e}\")\n",
    "            \n",
    "            with self.lock:\n",
    "                self.stats[\"api_errors\"] += 1\n",
    "            \n",
    "            return {\n",
    "                'Occupation_String': record['original'],\n",
    "                'rule_based_result': record['rule_based_result'],\n",
    "                'rule_based_confidence': record['confidence'],\n",
    "                'county': record.get('county', 'Unknown'),\n",
    "                'sex': record.get('sex', 'Unknown'),\n",
    "                'Occode_Desc': record.get('Occode_Desc', 'Unknown'),\n",
    "                'llm_corrected': record['rule_based_result'],\n",
    "                'is_corrected': False,\n",
    "                'final_result': record['rule_based_result'],\n",
    "                'processing_time': 0,\n",
    "                'llm_response_raw': f'Error: {str(e)}'\n",
    "            }\n",
    "    \n",
    "    def save_checkpoint_single(self, df: pd.DataFrame, processed_count: int, checkpoint_file: str):\n",
    "        \"\"\"Save checkpoint for single processing\"\"\"\n",
    "        checkpoint_data = {\n",
    "            \"processed_count\": processed_count,\n",
    "            \"stats\": self.stats,\n",
    "            \"timestamp\": time.time(),\n",
    "            \"processing_mode\": \"single_record\"\n",
    "        }\n",
    "        \n",
    "        # Save progress info\n",
    "        with open(checkpoint_file + '.json', 'w') as f:\n",
    "            json.dump(checkpoint_data, f, indent=2)\n",
    "        \n",
    "        # Save current data\n",
    "        df.to_csv(checkpoint_file + '.csv', index=False)\n",
    "        \n",
    "        logger.info(f\" Checkpoint saved: {processed_count:,} records processed\")\n",
    "    \n",
    "    def load_checkpoint_single(self, checkpoint_file: str) -> Tuple[Optional[pd.DataFrame], int]:\n",
    "        \"\"\"Load checkpoint for single processing\"\"\"\n",
    "        json_file = checkpoint_file + '.json'\n",
    "        csv_file = checkpoint_file + '.csv'\n",
    "        \n",
    "        if os.path.exists(json_file) and os.path.exists(csv_file):\n",
    "            # Load progress info\n",
    "            with open(json_file, 'r') as f:\n",
    "                checkpoint_data = json.load(f)\n",
    "            \n",
    "            # Load data\n",
    "            df = pd.read_csv(csv_file)\n",
    "            \n",
    "            self.stats = checkpoint_data.get(\"stats\", self.stats)\n",
    "            processed_count = checkpoint_data.get(\"processed_count\", 0)\n",
    "            \n",
    "            logger.info(f\"🔄 Resumed from checkpoint: {processed_count:,} records processed\")\n",
    "            return df, processed_count\n",
    "        \n",
    "        return None, 0\n",
    "    \n",
    "    def create_unique_records_map(self, df: pd.DataFrame, confidence_threshold: float) -> Tuple[pd.DataFrame, Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        NEW METHOD: Create unique records for processing and mapping back to original\n",
    "        \"\"\"\n",
    "        # Filter records that need processing\n",
    "        mask = df['confidence'] < confidence_threshold\n",
    "        to_process_df = df[mask].copy()\n",
    "        \n",
    "        if len(to_process_df) == 0:\n",
    "            logger.info(\" No records need processing\")\n",
    "            return pd.DataFrame(), {}\n",
    "        \n",
    "        logger.info(f\" Records below confidence threshold: {len(to_process_df):,}\")\n",
    "        \n",
    "        # Group by Occupation_String to find duplicates\n",
    "        occupation_groups = to_process_df.groupby('Occupation_String').agg({\n",
    "            'standardized': 'first',  # Take first standardized result\n",
    "            'confidence': 'first',    # Take first confidence score\n",
    "            'County': 'first',        # Take first county (for context)\n",
    "            'Sex': lambda x: x.mode().iloc[0] if not x.mode().empty else x.iloc[0],  # Take most common sex\n",
    "            'Occode_Desc': 'first'    # Take first description\n",
    "        }).reset_index()\n",
    "        \n",
    "        # Count duplicates\n",
    "        occupation_counts = to_process_df['Occupation_String'].value_counts()\n",
    "        \n",
    "        # Create mapping from unique occupation to final result\n",
    "        unique_to_final_map = {}\n",
    "        \n",
    "        # Add duplicate count to stats\n",
    "        total_duplicates = len(to_process_df) - len(occupation_groups)\n",
    "        self.stats[\"duplicate_records_saved\"] = total_duplicates\n",
    "        self.stats[\"unique_records_processed\"] = len(occupation_groups)\n",
    "        self.stats[\"deduplication_ratio\"] = total_duplicates / len(to_process_df) if len(to_process_df) > 0 else 0\n",
    "        \n",
    "        logger.info(f\" Unique occupation strings to process: {len(occupation_groups):,}\")\n",
    "        logger.info(f\" Duplicate records saved from processing: {total_duplicates:,}\")\n",
    "        logger.info(f\" Deduplication efficiency: {self.stats['deduplication_ratio']:.1%}\")\n",
    "        \n",
    "        # Show top duplicates\n",
    "        top_duplicates = occupation_counts.head(5)\n",
    "        if len(top_duplicates) > 0:\n",
    "            logger.info(\" Most frequent occupation strings:\")\n",
    "            for occupation, count in top_duplicates.items():\n",
    "                if count > 1:\n",
    "                    logger.info(f\"  '{occupation}': {count:,} occurrences\")\n",
    "        \n",
    "        return occupation_groups, unique_to_final_map\n",
    "    \n",
    "    def apply_unique_results_to_original(self, df: pd.DataFrame, unique_results: pd.DataFrame, \n",
    "                                       confidence_threshold: float) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        NEW METHOD: Apply results from unique processing back to original dataframe\n",
    "        \"\"\"\n",
    "        logger.info(\" Mapping unique results back to original dataset...\")\n",
    "        \n",
    "        # Create mapping from occupation string to final result\n",
    "        occupation_to_result = dict(zip(unique_results['Occupation_String'], unique_results['final_result']))\n",
    "        \n",
    "        # Add final_result column if not exists\n",
    "        if 'final_result' not in df.columns:\n",
    "            df['final_result'] = df['standardized'].copy()\n",
    "        \n",
    "        # Apply results to all matching occupation strings\n",
    "        mask = df['confidence'] < confidence_threshold\n",
    "        updated_count = 0\n",
    "        \n",
    "        for idx, row in df[mask].iterrows():\n",
    "            occupation = row['Occupation_String']\n",
    "            if occupation in occupation_to_result:\n",
    "                df.loc[idx, 'final_result'] = occupation_to_result[occupation]\n",
    "                updated_count += 1\n",
    "        \n",
    "        logger.info(f\" Applied results to {updated_count:,} records\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def process_large_dataset_single(self, \n",
    "                                   df: pd.DataFrame,\n",
    "                                   confidence_threshold: float = 0.8,\n",
    "                                   chunk_size: int = 20000,        # Large chunks for memory efficiency\n",
    "                                   save_interval: int = 1000,      # Save every 1000 records\n",
    "                                   checkpoint_file: str = \"ollama_single_checkpoint\",\n",
    "                                   resume_from_checkpoint: bool = True,\n",
    "                                   use_threading: bool = True,\n",
    "                                   use_unique_processing: bool = True) -> pd.DataFrame:  # NEW PARAMETER\n",
    "        \"\"\"\n",
    "        ENHANCED: Process large dataset with optimized single record processing and unique record deduplication\n",
    "        \"\"\"\n",
    "        \n",
    "        logger.info(\" Starting optimized single record processing with unique deduplication\")\n",
    "        logger.info(f\" Total records: {len(df):,}\")\n",
    "        logger.info(f\" Threading: {'Enabled' if use_threading else 'Disabled'} ({self.max_workers} workers)\")\n",
    "        logger.info(f\" Unique processing: {'Enabled' if use_unique_processing else 'Disabled'}\")\n",
    "        \n",
    "        # NEW: Create unique records if enabled\n",
    "        if use_unique_processing:\n",
    "            unique_df, unique_map = self.create_unique_records_map(df, confidence_threshold)\n",
    "            \n",
    "            if len(unique_df) == 0:\n",
    "                logger.info(\" All records have been processed or no records need processing\")\n",
    "                return df\n",
    "            \n",
    "            # Process unique records instead of full dataset\n",
    "            processing_df = unique_df.copy()\n",
    "        else:\n",
    "            # Original logic: process full dataset\n",
    "            processing_df = df.copy()\n",
    "        \n",
    "        # Try to resume from checkpoint\n",
    "        processed_count = 0\n",
    "        if resume_from_checkpoint:\n",
    "            loaded_df, processed_count = self.load_checkpoint_single(checkpoint_file)\n",
    "            if loaded_df is not None:\n",
    "                processing_df = loaded_df\n",
    "                logger.info(f\" Successfully resumed from checkpoint\")\n",
    "        \n",
    "        # Add final_result column if not exists\n",
    "        if 'final_result' not in processing_df.columns:\n",
    "            processing_df['final_result'] = processing_df['standardized'].copy()\n",
    "        \n",
    "        # Filter records that need processing (for unique processing, all records need processing)\n",
    "        if use_unique_processing:\n",
    "            to_process_indices = processing_df.index.tolist()\n",
    "        else:\n",
    "            mask = processing_df['confidence'] < confidence_threshold\n",
    "            to_process_indices = processing_df[mask].index.tolist()\n",
    "        \n",
    "        # Skip already processed records\n",
    "        if processed_count > 0:\n",
    "            to_process_indices = [idx for idx in to_process_indices if idx >= processed_count]\n",
    "        \n",
    "        logger.info(f\" Records to process: {len(to_process_indices):,}\")\n",
    "        logger.info(f\" Already processed: {processed_count:,}\")\n",
    "        \n",
    "        if len(to_process_indices) == 0:\n",
    "            logger.info(\" All records have been processed\")\n",
    "            if use_unique_processing:\n",
    "                # Map results back to original dataset\n",
    "                return self.apply_unique_results_to_original(df, processing_df, confidence_threshold)\n",
    "            else:\n",
    "                return processing_df\n",
    "        \n",
    "        # Estimate processing time\n",
    "        if self.stats[\"total_processed\"] > 0:\n",
    "            avg_time = self.stats[\"average_response_time\"]\n",
    "            estimated_time = avg_time * len(to_process_indices)\n",
    "            logger.info(f\"  Estimated remaining time: {estimated_time/3600:.1f} hours\")\n",
    "        \n",
    "        # Prepare records for processing\n",
    "        records_to_process = []\n",
    "        for idx in to_process_indices:\n",
    "            row = processing_df.loc[idx]\n",
    "            record = {\n",
    "                'index': idx,\n",
    "                'original': row['Occupation_String'],\n",
    "                'rule_based_result': row['standardized'],\n",
    "                'confidence': row['confidence'],\n",
    "                'county': row.get('County', 'Unknown'),\n",
    "                'sex': row.get('Sex', 'Unknown'),\n",
    "                'Occode_Desc': row.get('Occode_Desc', 'Unknown')\n",
    "            }\n",
    "            records_to_process.append(record)\n",
    "        \n",
    "        # Process records\n",
    "        try:\n",
    "            if use_threading and self.max_workers > 1:\n",
    "                self._process_records_threaded(processing_df, records_to_process, processed_count, save_interval, checkpoint_file)\n",
    "            else:\n",
    "                self._process_records_sequential(processing_df, records_to_process, processed_count, save_interval, checkpoint_file)\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\" Processing interrupted, saving current progress...\")\n",
    "            self.save_checkpoint_single(processing_df, processed_count, checkpoint_file)\n",
    "            raise\n",
    "        \n",
    "        # Final save\n",
    "        self.save_checkpoint_single(processing_df, len(to_process_indices), checkpoint_file)\n",
    "        \n",
    "        # NEW: If using unique processing, map results back to original dataset\n",
    "        if use_unique_processing:\n",
    "            final_df = self.apply_unique_results_to_original(df, processing_df, confidence_threshold)\n",
    "            logger.info(\" Optimized unique record processing completed!\")\n",
    "        else:\n",
    "            final_df = processing_df\n",
    "            logger.info(\" Optimized single record processing completed!\")\n",
    "        \n",
    "        self.print_final_statistics()\n",
    "        \n",
    "        return final_df\n",
    "    \n",
    "    def _process_records_sequential(self, df: pd.DataFrame, records: List[Dict],\n",
    "                                  processed_count: int, save_interval: int, checkpoint_file: str):\n",
    "        \"\"\"Process records sequentially\"\"\"\n",
    "        current_count = processed_count\n",
    "        \n",
    "        for record in tqdm(records, desc=\"Processing records\"):\n",
    "            enhanced_record = self.process_single_record_optimized(record)\n",
    "            \n",
    "            # Update DataFrame\n",
    "            idx = record['index']\n",
    "            df.loc[idx, 'final_result'] = enhanced_record['final_result']\n",
    "            \n",
    "            current_count += 1\n",
    "            \n",
    "            # Save checkpoint periodically\n",
    "            if current_count % save_interval == 0:\n",
    "                self.save_checkpoint_single(df, current_count, checkpoint_file)\n",
    "            \n",
    "            # Delay between requests\n",
    "            if self.delay_between_requests > 0:\n",
    "                time.sleep(self.delay_between_requests)\n",
    "    \n",
    "    def _process_records_threaded(self, df: pd.DataFrame, records: List[Dict],\n",
    "                                processed_count: int, save_interval: int, checkpoint_file: str):\n",
    "        \"\"\"Process records with threading\"\"\"\n",
    "        current_count = processed_count\n",
    "        processed_indices = []\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            # Submit all records\n",
    "            future_to_record = {\n",
    "                executor.submit(self.process_single_record_optimized, record): record\n",
    "                for record in records\n",
    "            }\n",
    "            \n",
    "            # Process completed results\n",
    "            for future in tqdm(as_completed(future_to_record), total=len(records), desc=\"Processing records\"):\n",
    "                record = future_to_record[future]\n",
    "                \n",
    "                try:\n",
    "                    enhanced_record = future.result()\n",
    "                    \n",
    "                    # Update DataFrame\n",
    "                    idx = record['index']\n",
    "                    df.loc[idx, 'final_result'] = enhanced_record['final_result']\n",
    "                    \n",
    "                    processed_indices.append(idx)\n",
    "                    current_count += 1\n",
    "                    \n",
    "                    # Save checkpoint periodically\n",
    "                    if current_count % save_interval == 0:\n",
    "                        self.save_checkpoint_single(df, current_count, checkpoint_file)\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Record processing failed: {e}\")\n",
    "                \n",
    "                # Small delay for rate limiting\n",
    "                if self.delay_between_requests > 0:\n",
    "                    time.sleep(self.delay_between_requests)\n",
    "    \n",
    "    def print_final_statistics(self):\n",
    "        \"\"\"Print comprehensive final statistics\"\"\"\n",
    "        logger.info(\"\\n\" + \"=\"*60)\n",
    "        logger.info(\" OPTIMIZED SINGLE PROCESSING STATISTICS\")\n",
    "        logger.info(\"=\"*60)\n",
    "        logger.info(f\"Total records processed: {self.stats['total_processed']:,}\")\n",
    "        logger.info(f\"Total corrections made: {self.stats['corrections_made']:,}\")\n",
    "        logger.info(f\"API errors: {self.stats['api_errors']:,}\")\n",
    "        \n",
    "        # NEW: Unique processing statistics\n",
    "        if self.stats['unique_records_processed'] > 0:\n",
    "            logger.info(f\"Unique records processed: {self.stats['unique_records_processed']:,}\")\n",
    "            logger.info(f\"Duplicate records saved: {self.stats['duplicate_records_saved']:,}\")\n",
    "            logger.info(f\"Deduplication efficiency: {self.stats['deduplication_ratio']:.1%}\")\n",
    "        \n",
    "        if self.stats['total_processed'] > 0:\n",
    "            success_rate = (self.stats['total_processed'] - self.stats['api_errors']) / self.stats['total_processed'] * 100\n",
    "            correction_rate = self.stats['corrections_made'] / self.stats['total_processed'] * 100\n",
    "            avg_speed = self.stats['total_processed'] / self.stats['processing_time'] if self.stats['processing_time'] > 0 else 0\n",
    "            \n",
    "            logger.info(f\"Success rate: {success_rate:.1f}%\")\n",
    "            logger.info(f\"Correction rate: {correction_rate:.1f}%\")\n",
    "            logger.info(f\"Average processing speed: {avg_speed:.1f} records/second\")\n",
    "            logger.info(f\"Average response time: {self.stats['average_response_time']:.2f} seconds\")\n",
    "            logger.info(f\"Total processing time: {self.stats['processing_time']/3600:.2f} hours\")\n",
    "        \n",
    "        logger.info(\"=\"*60)\n",
    "\n",
    "# Enhanced convenience function with unique processing\n",
    "def process_occupation_data_single_optimized(df_cleaned: pd.DataFrame,\n",
    "                                           model_name: str = \"Qwen2.5:7b-instruct\",\n",
    "                                           confidence_threshold: float = 0.8,\n",
    "                                           chunk_size: int = 20000,\n",
    "                                           save_interval: int = 1000,\n",
    "                                           delay: float = 0.1,\n",
    "                                           output_file: str = \"df_cleaned_single_optimized.csv\",\n",
    "                                           resume_from_checkpoint: bool = True,\n",
    "                                           use_threading: bool = True,\n",
    "                                           max_workers: int = 6,\n",
    "                                           use_unique_processing: bool = True) -> pd.DataFrame:  # NEW PARAMETER\n",
    "    \"\"\"\n",
    "    ENHANCED: Optimized single record processing function with unique processing\n",
    "    \n",
    "    Args:\n",
    "        df_cleaned: Cleaned DataFrame\n",
    "        model_name: Ollama model name\n",
    "        confidence_threshold: Confidence threshold for processing\n",
    "        chunk_size: Records per processing chunk\n",
    "        save_interval: Checkpoint save interval\n",
    "        delay: Delay between requests (very small for single processing)\n",
    "        output_file: Output file name\n",
    "        resume_from_checkpoint: Whether to resume from checkpoint\n",
    "        use_threading: Whether to use multi-threading\n",
    "        max_workers: Number of concurrent workers\n",
    "        use_unique_processing: NEW - Whether to deduplicate by Occupation_String before processing\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(\" Starting optimized single record Ollama processing with unique deduplication\")\n",
    "    logger.info(f\" Input data: {len(df_cleaned):,} records\")\n",
    "    logger.info(f\" Single record mode with {max_workers} workers\")\n",
    "    logger.info(f\" Threading: {'Enabled' if use_threading else 'Disabled'}\")\n",
    "    logger.info(f\" Unique processing: {'Enabled' if use_unique_processing else 'Disabled'}\")\n",
    "    \n",
    "    # Check required columns\n",
    "    required_columns = ['Occupation_String', 'standardized', 'confidence']\n",
    "    missing_columns = [col for col in required_columns if col not in df_cleaned.columns]\n",
    "    if missing_columns:\n",
    "        raise ValueError(f\"Missing required columns: {missing_columns}\")\n",
    "    \n",
    "    # Check auxiliary columns\n",
    "    auxiliary_columns = []\n",
    "    for col in ['County', 'Sex', 'Occode_Desc']:\n",
    "        if col in df_cleaned.columns:\n",
    "            auxiliary_columns.append(col)\n",
    "    \n",
    "    logger.info(f\" Available auxiliary columns: {auxiliary_columns}\")\n",
    "    \n",
    "    # NEW: Show deduplication potential analysis\n",
    "    if use_unique_processing:\n",
    "        mask = df_cleaned['confidence'] < confidence_threshold\n",
    "        to_process_df = df_cleaned[mask]\n",
    "        \n",
    "        if len(to_process_df) > 0:\n",
    "            unique_occupations = to_process_df['Occupation_String'].nunique()\n",
    "            total_to_process = len(to_process_df)\n",
    "            potential_savings = total_to_process - unique_occupations\n",
    "            savings_percentage = (potential_savings / total_to_process) * 100 if total_to_process > 0 else 0\n",
    "            \n",
    "            logger.info(f\"🔍 Deduplication analysis:\")\n",
    "            logger.info(f\"  Total records to process: {total_to_process:,}\")\n",
    "            logger.info(f\"  Unique occupation strings: {unique_occupations:,}\")\n",
    "            logger.info(f\"  Potential processing savings: {potential_savings:,} ({savings_percentage:.1f}%)\")\n",
    "            \n",
    "    \n",
    "    # Create optimized single processor\n",
    "    processor = OptimizedSingleProcessor(\n",
    "        model_name=model_name,\n",
    "        delay_between_requests=delay,\n",
    "        max_workers=max_workers\n",
    "    )\n",
    "    \n",
    "    # Process data with single record optimization and unique processing\n",
    "    df_result = processor.process_large_dataset_single(\n",
    "        df_cleaned,\n",
    "        confidence_threshold=confidence_threshold,\n",
    "        chunk_size=chunk_size,\n",
    "        save_interval=save_interval,\n",
    "        resume_from_checkpoint=resume_from_checkpoint,\n",
    "        use_threading=use_threading,\n",
    "        use_unique_processing=use_unique_processing  # NEW PARAMETER\n",
    "    )\n",
    "    \n",
    "    # final_result to lowercase and remove all puncuation\n",
    "    df_result['final_result'] = df_result['final_result'].str.lower().str.replace(f\"[{string.punctuation}]\", \"\", regex=True)\n",
    "\n",
    "    # Save final results\n",
    "    df_result.to_csv(output_file, index=False)\n",
    "    logger.info(f\" Final results saved to: {output_file}\")\n",
    "    \n",
    "    # Show correction examples\n",
    "    corrections_mask = df_result['final_result'] != df_result['standardized']\n",
    "    corrections = df_result[corrections_mask]\n",
    "    \n",
    "    if len(corrections) > 0:\n",
    "        logger.info(f\"\\n Correction examples (total: {len(corrections):,}):\")\n",
    "        for _, row in corrections.head(5).iterrows():\n",
    "            logger.info(f\"  '{row['Occupation_String']}' → '{row['final_result']}'\")\n",
    "            logger.info(f\"    (Original: {row['standardized']})\")\n",
    "    \n",
    "    # NEW: Show unique processing efficiency\n",
    "    if use_unique_processing and processor.stats['unique_records_processed'] > 0:\n",
    "        total_that_would_process = len(df_cleaned[df_cleaned['confidence'] < confidence_threshold])\n",
    "        actual_processed = processor.stats['unique_records_processed']\n",
    "        saved_processing = total_that_would_process - actual_processed\n",
    "        \n",
    "        logger.info(f\"\\n Unique Processing Efficiency:\")\n",
    "        logger.info(f\"  Would have processed: {total_that_would_process:,} records\")\n",
    "        logger.info(f\"  Actually processed: {actual_processed:,} unique records\")\n",
    "        logger.info(f\"  Processing saved: {saved_processing:,} records ({(saved_processing/total_that_would_process)*100:.1f}%)\")\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    df_llm_check = pd.read_csv(\"D:\\\\Postgraduate\\\\Data Science Project\\\\Code\\\\cleaning_results_1851_validation\\\\llm_check_needed.csv\",header=0) \n",
    "    print(\" Dataset overview:\")\n",
    "    print(f\"Total records: {len(df_llm_check):,}\")\n",
    "    print(f\"Low confidence records: {len(df_llm_check[df_llm_check['confidence'] < 0.8]):,}\")\n",
    "    print()\n",
    "    \n",
    "    try:\n",
    "        # Enhanced single record processing with unique deduplication\n",
    "        df_final = process_occupation_data_single_optimized(\n",
    "            df_llm_check,\n",
    "            model_name=\"Qwen2.5:7b-instruct\",\n",
    "            confidence_threshold=0.8,\n",
    "            chunk_size=20000,                # Large chunks for memory efficiency\n",
    "            save_interval=1000,              # Save every 1000 records\n",
    "            delay=0.1,                       # Fast single processing\n",
    "            output_file=\".\\\\cleaning_results_1851_validation\\\\df_llm_check_single_1911.csv\",\n",
    "            resume_from_checkpoint=True,     # Support resumption\n",
    "            use_threading=True,              # Enable multi-threading\n",
    "            max_workers=6,                   # 6 concurrent workers for single requests\n",
    "            use_unique_processing=True       # NEW: Enable unique processing\n",
    "        )        \n",
    "\n",
    "        # Show performance statistics\n",
    "        total_corrections = len(df_final[df_final['final_result'] != df_final['standardized']])\n",
    "        print(f\" Total corrections made: {total_corrections:,}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Processing failed: {e}\")\n",
    "        print(\"\\n Troubleshooting:\")\n",
    "        print(\"1. Ensure Ollama is running: ollama serve\")\n",
    "        print(\"2. Ensure model is downloaded: ollama pull Qwen2.5:7b-instruct\")\n",
    "        print(\"3. Check system resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_llm_final = pd.read_csv(\".\\\\cleaning_results_1851_validation\\\\df_llm_check_single_1911.csv\",header=0)\n",
    "df_rule_based = pd.read_csv(\".\\\\cleaning_results_1851_validation\\\\cleaned_occupations_with_categories.csv\", header = 0)\n",
    "# merge df_llm_final to df_rule_based，by County, Sex, Occupation_String, Occode, HISCO_x, HISCO_y\n",
    "df_merged = pd.merge(\n",
    "    df_rule_based,\n",
    "    df_llm_final[['County', 'Sex', 'Occupation_String', 'Occode', 'HISCO_x', 'HISCO_y', 'final_result', 'standardized']],\n",
    "    on=['County', 'Sex', 'Occupation_String', 'Occode', 'HISCO_x', 'HISCO_y'],\n",
    "    how='left',\n",
    "    suffixes=('', '_llm')\n",
    ")\n",
    "\n",
    "# if final_result is NaN，then use standardized to filled\n",
    "df_merged['final_result'] = df_merged['final_result'].fillna(df_merged['standardized'])\n",
    "df_merged.to_csv(\".\\\\cleaning_results_1851_validation\\\\cleaned_occupation_with_llm.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before aggregation: 1,000 records\n",
      "Found 4 groups with 8 records that can be aggregated\n",
      "After aggregation: 996 records\n",
      "Reduction: 4 records (0.4%)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "def aggregate_similar_occupations(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate occupations with same County, Sex, Occode, and standardized values\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with final_result occupations\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with aggregated records\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = ['County', 'Sex', 'Occode', 'final_result', 'Count']\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing required columns for aggregation: {missing_cols}\")\n",
    "        return df\n",
    "    \n",
    "    print(f\"Before aggregation: {len(df):,} records\")\n",
    "    \n",
    "    # Define grouping columns\n",
    "    group_columns = ['County', 'Sex', 'Occode', 'final_result']\n",
    "    \n",
    "    # Check for records that can be aggregated\n",
    "    pre_agg_check = df.groupby(group_columns).size()\n",
    "    aggregatable_groups = (pre_agg_check > 1).sum()\n",
    "    total_aggregatable_records = pre_agg_check[pre_agg_check > 1].sum()\n",
    "    \n",
    "    print(f\"Found {aggregatable_groups:,} groups with {total_aggregatable_records:,} records that can be aggregated\")\n",
    "    \n",
    "    # Prepare aggregation dictionary\n",
    "    agg_dict = {'Count': 'sum'}  # Sum the counts\n",
    "    \n",
    "    # For other columns, take the first value\n",
    "    other_columns = [col for col in df.columns if col not in group_columns + ['Count']]\n",
    "    for col in other_columns:\n",
    "        agg_dict[col] = 'first'\n",
    "    \n",
    "    # Perform aggregation\n",
    "    try:\n",
    "        df_aggregated = df.groupby(group_columns, as_index=False).agg(agg_dict)\n",
    "        \n",
    "        # Sort by Count descending to put most frequent occupations first\n",
    "        df_aggregated = df_aggregated.sort_values(['County', 'Count'], ascending=[True, False])\n",
    "        \n",
    "        # Reset index\n",
    "        df_aggregated = df_aggregated.reset_index(drop=True)\n",
    "        \n",
    "        # Calculate aggregation statistics\n",
    "        original_count = len(df)\n",
    "        aggregated_count = len(df_aggregated)\n",
    "        reduction_rate = (original_count - aggregated_count) / original_count\n",
    "        \n",
    "        print(f\"After aggregation: {aggregated_count:,} records\")\n",
    "        print(f\"Reduction: {original_count - aggregated_count:,} records ({reduction_rate:.1%})\")\n",
    "\n",
    "        \n",
    "        return df_aggregated\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during aggregation: {e}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "df_aggregated = aggregate_similar_occupations(df_merged)\n",
    "\n",
    "agg_output = os.path.join(\"./cleaning_results_1851_validation\", \"aggregated_occupations.csv\")\n",
    "df_aggregated.to_csv(agg_output, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Occupation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " COMPLETE MULTI-OCCUPATION PROCESSING WORKFLOW\n",
      "============================================================\n",
      "\n",
      " CREATING SINGLE OCCUPATION DATASET\n",
      "==================================================\n",
      " Single occupation dataset created:\n",
      "   Original multi-occupation records: 30\n",
      "   Total single occupations extracted: 60\n",
      "   Average occupations per record: 2.0\n",
      "\n",
      "🔍 Sample splitting results:\n",
      "   1. 'brace bit and gunblet mak'\n",
      "      → ['brace bit', 'gunblet mak']\n",
      "   2. 'teacher classical mathematical and commercial'\n",
      "      → ['teacher classical mathematical', 'commercial']\n",
      "   3. 'groser and weaver wife'\n",
      "      → ['groser', 'weaver wife']\n",
      "\n",
      " PREPARING FOR OCCANINE PREDICTION\n",
      "==================================================\n",
      " OccCANINE input prepared:\n",
      "   Records for prediction: 60\n",
      "   Enhanced input format: True\n",
      "\n",
      " Sample inputs for OccCANINE:\n",
      "   1. Male brace bit in Yorkshire West Riding\n",
      "   2. Male gunblet mak in Yorkshire West Riding\n",
      "   3. Male teacher classical mathematical in Yorkshire West Riding\n",
      "Processed batch 1 out of 1 batchesProduced HISCO codes for 60 observations in 0 hours, 0 minutes and 0.875 seconds.\n",
      "Estimated hours saved compared to human labeller (assuming 10 seconds per label):\n",
      " ---> 0 hours, 9 minutes and 59 seconds\n",
      " MERGING OCCANINE PREDICTIONS\n",
      "========================================\n",
      " Loaded predictions from: ./cleaning_results_1851_validation/multi_OccCANINE_predictions.csv\n",
      "   Prediction records: 60\n",
      "\n",
      " MERGING PREDICTIONS BACK TO ORIGINAL DATA\n",
      "==================================================\n",
      " Merging results:\n",
      "   Original multi-occupation records: 30\n",
      "   Successfully processed: 30\n",
      "   Records with merged predictions: 30\n",
      "\n",
      " Prediction count distribution:\n",
      "   2 predictions: 30 records\n",
      "\n",
      " Sample merged results:\n",
      "   1. 'brace bit and gunblet mak'\n",
      "      → All HISCO codes: [[61220, np.float32(0.4332489), 'Field Crop Farmer']]; []\n",
      "      → Total predictions: 2\n",
      "   2. 'teacher classical mathematical and commercial'\n",
      "      → All HISCO codes: [[13100, np.float32(0.32729658), 'University and Higher Education Teacher, Subject Unknown'], [13990, np.float32(0.9563595), 'Other Teachers']]; [[61220, np.float32(0.69158006), 'Field Crop Farmer']]\n",
      "      → Total predictions: 2\n",
      "   3. 'groser and weaver wife'\n",
      "      → All HISCO codes: [[45190, np.float32(0.35304916), 'Other Salesmen, Shop Assistants and Demonstrators']]; [[13990, np.float32(0.2894419), 'Other Teachers'], [14120, np.float32(0.8824011), 'Minister of Religion']]\n",
      "      → Total predictions: 2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "class MultiOccupationProcessor:\n",
    "    \n",
    "    def __init__(self):\n",
    "        # connection patterns\n",
    "        self.connection_patterns = [\n",
    "            r'\\s+and\\s+',           # and\n",
    "            r'\\s+&\\s+',             # &\n",
    "            r'\\s+or\\s+',            # or\n",
    "            r'\\s*,\\s*',             # comma\n",
    "            r'\\s*/\\s*',             # slash\n",
    "        ]\n",
    "        \n",
    "        # non_occupation_terms\n",
    "        self.non_occupation_terms = {\n",
    "            'pauper', 'unemployed', 'invalid', 'disabled', 'retired', \n",
    "            'deceased', 'widow', 'widower', 'single', 'married',\n",
    "            'at home', 'home', 'at school', 'school'\n",
    "        }\n",
    "        \n",
    "        # modifier terms\n",
    "        self.modifier_terms = {\n",
    "            'former', 'ex', 'retired', 'late', 'widow of', 'wife of',\n",
    "            'son of', 'daughter of', 'employing', 'employed by'\n",
    "        }\n",
    "    \n",
    "    def split_occupation_string(self, occupation_string: str) -> List[str]:\n",
    "        \"\"\"Split occupation string into single occupations\"\"\"\n",
    "        if pd.isna(occupation_string):\n",
    "            return []\n",
    "        \n",
    "        text = str(occupation_string).lower().strip()\n",
    "        \n",
    "        # split by connection patterns\n",
    "        parts = [text]\n",
    "        for pattern in self.connection_patterns:\n",
    "            new_parts = []\n",
    "            for part in parts:\n",
    "                new_parts.extend(re.split(pattern, part))\n",
    "            parts = new_parts\n",
    "        \n",
    "        # clean parts\n",
    "        cleaned_parts = []\n",
    "        for part in parts:\n",
    "            part = part.strip()\n",
    "            if part and part not in self.non_occupation_terms:\n",
    "                # remove modifier terms\n",
    "                for modifier in self.modifier_terms:\n",
    "                    part = re.sub(rf'\\b{re.escape(modifier)}\\b', '', part)\n",
    "                \n",
    "                part = re.sub(r'\\s+', ' ', part).strip()\n",
    "                \n",
    "                if part and len(part) > 2:  \n",
    "                    cleaned_parts.append(part)\n",
    "        \n",
    "        return cleaned_parts\n",
    "    \n",
    "    def create_single_occupation_dataset(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Create single occupation dataset for OccCANINE prediction\"\"\"\n",
    "        print(\"\\n CREATING SINGLE OCCUPATION DATASET\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        single_occupation_records = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            occupation_parts = self.split_occupation_string(row['Occupation_String'])\n",
    "            \n",
    "            for i, part in enumerate(occupation_parts):\n",
    "                single_record = {\n",
    "                    'original_index': idx,\n",
    "                    'original_occupation_string': row['Occupation_String'],\n",
    "                    'split_index': i,\n",
    "                    'single_occupation': part,\n",
    "                    'County': row.get('County', ''),\n",
    "                    'Sex': row.get('Sex', ''),\n",
    "                    'Year': row.get('Year', ''),\n",
    "                    'Count': row.get('Count', 1),\n",
    "                    'original_hisco': row.get('HISCO_y', ''),\n",
    "                    'category': row.get('category', ''),\n",
    "                    'confidence': row.get('confidence', 0)\n",
    "                }\n",
    "                single_occupation_records.append(single_record)\n",
    "        \n",
    "        single_df = pd.DataFrame(single_occupation_records)\n",
    "        \n",
    "        print(f\" Single occupation dataset created:\")\n",
    "        print(f\"   Original multi-occupation records: {len(df):,}\")\n",
    "        print(f\"   Total single occupations extracted: {len(single_df):,}\")\n",
    "        print(f\"   Average occupations per record: {len(single_df)/len(df):.1f}\")\n",
    "        \n",
    "        # Show sample splitting results\n",
    "        print(f\"\\n🔍 Sample splitting results:\")\n",
    "        sample_original = df.head(3)\n",
    "        for i, (_, row) in enumerate(sample_original.iterrows(), 1):\n",
    "            original_idx = row.name\n",
    "            splits = single_df[single_df['original_index'] == original_idx]['single_occupation'].tolist()\n",
    "            print(f\"   {i}. '{row['Occupation_String']}'\")\n",
    "            print(f\"      → {splits}\")\n",
    "        \n",
    "        return single_df\n",
    "    \n",
    "    def prepare_for_occanine(self, single_df: pd.DataFrame, \n",
    "                           enhanced_input: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"Prepare input data for OccCANINE prediction\"\"\"\n",
    "        print(\"\\n PREPARING FOR OCCANINE PREDICTION\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        def create_input_string(row):\n",
    "            \"\"\"Create OccCANINE input string\"\"\"\n",
    "            if enhanced_input:\n",
    "                # Enhanced format: Sex + Occupation + in + County\n",
    "                sex = str(row['Sex']).title() if pd.notna(row['Sex']) else \"\"\n",
    "                occupation = str(row['single_occupation']).lower()\n",
    "                county = str(row['County']).title() if pd.notna(row['County']) else \"\"\n",
    "                \n",
    "                parts = []\n",
    "                if sex:\n",
    "                    parts.append(sex)\n",
    "                if occupation:\n",
    "                    parts.append(occupation)\n",
    "                \n",
    "                base_string = \" \".join(parts)\n",
    "                \n",
    "                if county:\n",
    "                    return f\"{base_string} in {county}\"\n",
    "                else:\n",
    "                    return base_string\n",
    "            else:\n",
    "                # Basic format: occupation only\n",
    "                return str(row['single_occupation']).lower()\n",
    "        \n",
    "        # Create input strings\n",
    "        single_df['occanine_input'] = single_df.apply(create_input_string, axis=1)\n",
    "        \n",
    "        # Create CSV file for OccCANINE\n",
    "        occanine_input = pd.DataFrame({\n",
    "            'record_id': range(len(single_df)),\n",
    "            'original_index': single_df['original_index'],\n",
    "            'split_index': single_df['split_index'],\n",
    "            'input_text': single_df['occanine_input'],\n",
    "            'original_occupation': single_df['single_occupation'],\n",
    "            'county': single_df['County'],\n",
    "            'sex': single_df['Sex']\n",
    "        })\n",
    "        \n",
    "        print(f\" OccCANINE input prepared:\")\n",
    "        print(f\"   Records for prediction: {len(occanine_input):,}\")\n",
    "        print(f\"   Enhanced input format: {enhanced_input}\")\n",
    "        \n",
    "        # Show sample inputs\n",
    "        print(f\"\\n Sample inputs for OccCANINE:\")\n",
    "        for i, (_, row) in enumerate(occanine_input.head(3).iterrows(), 1):\n",
    "            print(f\"   {i}. {row['input_text']}\")\n",
    "        \n",
    "        return occanine_input\n",
    "\n",
    "    def merge_predictions_back(self, original_df: pd.DataFrame, \n",
    "                             single_predictions: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Merge single occupation prediction results back to original data\"\"\"\n",
    "        print(\"\\n MERGING PREDICTIONS BACK TO ORIGINAL DATA\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        # Add new columns to original data\n",
    "        result_df = original_df.copy()\n",
    "        result_df['all_predicted_hisco'] = None\n",
    "        result_df['prediction_count'] = 0\n",
    "        \n",
    "        # Group prediction results by original index\n",
    "        prediction_groups = single_predictions.groupby('original_index')\n",
    "        \n",
    "        processed_count = 0\n",
    "        \n",
    "        for original_idx, group in prediction_groups:\n",
    "            if original_idx not in result_df.index:\n",
    "                continue\n",
    "            \n",
    "            # Filter valid predictions (only need predicted_hisco not null)\n",
    "            valid_predictions = group[\n",
    "                group['predicted_hisco'].notna()\n",
    "            ].copy()\n",
    "            \n",
    "            if len(valid_predictions) == 0:\n",
    "                result_df.loc[original_idx, 'all_predicted_hisco'] = 'no_valid_predictions'\n",
    "                result_df.loc[original_idx, 'prediction_count'] = 0\n",
    "                continue\n",
    "            \n",
    "            # Collect all prediction results\n",
    "            all_hisco_codes = []\n",
    "            all_predictions_detail = []\n",
    "            \n",
    "            for _, pred_row in valid_predictions.iterrows():\n",
    "                hisco_code = pred_row['predicted_hisco']\n",
    "                all_hisco_codes.append(str(hisco_code))\n",
    "                \n",
    "                # Save detailed information (including original occupation name if available)\n",
    "                detail = {\n",
    "                    'hisco': hisco_code,\n",
    "                    'split_index': pred_row.get('split_index', ''),\n",
    "                    'occupation': pred_row.get('original_occupation', '')\n",
    "                }\n",
    "                all_predictions_detail.append(detail)\n",
    "            \n",
    "            # Concatenate all HISCO codes into string (semicolon separated)\n",
    "            result_df.loc[original_idx, 'all_predicted_hisco'] = '; '.join(all_hisco_codes)\n",
    "            result_df.loc[original_idx, 'prediction_count'] = len(all_hisco_codes)\n",
    "            result_df.loc[original_idx, 'prediction_details'] = str(all_predictions_detail)\n",
    "            \n",
    "            processed_count += 1\n",
    "        \n",
    "        print(f\" Merging results:\")\n",
    "        print(f\"   Original multi-occupation records: {len(prediction_groups)}\")\n",
    "        print(f\"   Successfully processed: {processed_count}\")\n",
    "        \n",
    "        # Statistics of merging results\n",
    "        successful_merges = result_df['all_predicted_hisco'].notna().sum()\n",
    "        records_with_predictions = (result_df['all_predicted_hisco'] != 'no_valid_predictions').sum()\n",
    "        print(f\"   Records with merged predictions: {records_with_predictions}\")\n",
    "        \n",
    "        # Statistics of prediction count distribution\n",
    "        prediction_counts = result_df['prediction_count'].value_counts().sort_index()\n",
    "        print(f\"\\n Prediction count distribution:\")\n",
    "        for count, freq in prediction_counts.head(10).items():\n",
    "            print(f\"   {count} predictions: {freq} records\")\n",
    "        \n",
    "        if processed_count > 0:\n",
    "            print(f\"\\n Sample merged results:\")\n",
    "            sample_merged = result_df[\n",
    "                (result_df['all_predicted_hisco'].notna()) & \n",
    "                (result_df['all_predicted_hisco'] != 'no_valid_predictions')\n",
    "            ].head(3)\n",
    "            \n",
    "            for i, (_, row) in enumerate(sample_merged.iterrows(), 1):\n",
    "                print(f\"   {i}. '{row['Occupation_String']}'\")\n",
    "                print(f\"      → All HISCO codes: {row['all_predicted_hisco']}\")\n",
    "                print(f\"      → Total predictions: {row['prediction_count']}\")\n",
    "        \n",
    "        return result_df\n",
    "    \n",
    "def process_multi_occupations_complete_workflow(df: pd.DataFrame,\n",
    "                                              output_dir: str = \"./cleaning_results_1861\",\n",
    "                                              enhanced_input: bool = True,\n",
    "                                              ) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Complete multi-occupation processing workflow\n",
    "    \n",
    "    Args:\n",
    "        df: Original data DataFrame\n",
    "        output_dir: Output directory\n",
    "        enhanced_input: Whether to use enhanced input format\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (merged data, single occupation prediction data, evaluation results)\n",
    "    \"\"\"\n",
    "    \n",
    "    import os\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(\" COMPLETE MULTI-OCCUPATION PROCESSING WORKFLOW\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = MultiOccupationProcessor()\n",
    "    \n",
    "    # Create single occupation dataset\n",
    "    single_occupation_df = processor.create_single_occupation_dataset(df)\n",
    "    \n",
    "    # Step 3: Prepare OccCANINE input\n",
    "    occanine_input_df = processor.prepare_for_occanine(single_occupation_df, enhanced_input)\n",
    "    \n",
    "    # Save OccCANINE input file\n",
    "    input_file_path = os.path.join(output_dir, \"multi2single_occupations_for_occanine.csv\")\n",
    "    occanine_input_df.to_csv(input_file_path, index=False)\n",
    "    \n",
    "    return occanine_input_df, {}\n",
    "\n",
    "def merge_occanine_predictions(original_df: pd.DataFrame,\n",
    "                             prediction_file: str,\n",
    "                             ) -> Tuple[pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Merge OccCANINE prediction results to original data\n",
    "    \n",
    "    Args:\n",
    "        original_df: Original data (multi-occupation identified)\n",
    "        prediction_file: OccCANINE prediction result file path\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (merged data, evaluation results)\n",
    "    \"\"\"\n",
    "    print(\" MERGING OCCANINE PREDICTIONS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Load prediction results\n",
    "    try:\n",
    "        predictions_df = pd.read_csv(prediction_file)\n",
    "        print(f\" Loaded predictions from: {prediction_file}\")\n",
    "        print(f\"   Prediction records: {len(predictions_df):,}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading predictions: {e}\")\n",
    "        return original_df, {}\n",
    "    \n",
    "    # Initialize processor\n",
    "    processor = MultiOccupationProcessor()\n",
    "    \n",
    "    # Merge prediction results\n",
    "    merged_df = processor.merge_predictions_back(original_df, predictions_df)\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    df = pd.read_csv(\"D:\\\\Postgraduate\\\\Data Science Project\\\\Code\\\\cleaning_results_1851_validation\\\\multi_occupation_flagged.csv\",header=0)\n",
    "\n",
    "    single_occs, _ = process_multi_occupations_complete_workflow(df)\n",
    "    \n",
    "    single_occs_array = single_occs['input_text']\n",
    "    from histocc import OccCANINE\n",
    "\n",
    "    model = OccCANINE()\n",
    "    model.verbose = True\n",
    "    x = model.predict(\n",
    "        single_occs_array,\n",
    "        get_dict = True,\n",
    "        lang = \"en\"\n",
    "    )\n",
    "    \n",
    "    single_occs['predicted_hisco'] = x\n",
    "\n",
    "    single_occs.to_csv(\".\\\\cleaning_results_1851_validation\\\\multi_OccCANINE_predictions.csv\",index=False)\n",
    "\n",
    "    merged_df = merge_occanine_predictions(original_df=df,\n",
    "                                                       prediction_file='./cleaning_results_1851_validation/multi_OccCANINE_predictions.csv')\n",
    "    merged_df.to_csv(\"D:\\\\Postgraduate\\\\Data Science Project\\\\Code\\\\cleaning_results_1851_validation\\\\OccCANINE_result_multiple.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create InputText with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ENHANCING OCCUPATION INPUTS FOR OCCANINE\n",
      "============================================================\n",
      " Required columns found: ['final_result']\n",
      " Available optional columns: ['Sex', 'County', 'HISCO_x']\n",
      " CREATING ENHANCED INPUT STRINGS\n",
      "==================================================\n",
      "Format: 'Sex final_result in County'\n",
      "\n",
      " Enhancement Statistics:\n",
      "   Total strings created: 996\n",
      "   Average length: 39.5 characters\n",
      "   Max length: 100 characters\n",
      "   Min length: 18 characters\n",
      "\n",
      " Component Coverage:\n",
      "   With gender: 976 (98.0%)\n",
      "   With description: 0 (0.0%)\n",
      "   With location: 996 (100.0%)\n",
      "\n",
      " Sample Enhanced Strings:\n",
      "   1. Original: dressmaker\n",
      "      Enhanced: Male dressmaker in Anglesey\n",
      "\n",
      "   2. Original: retired draper\n",
      "      Enhanced: Male retired draper in Anglesey\n",
      "\n",
      "   3. Original: clogger barber\n",
      "      Enhanced: Male clogger barber in Anglesey\n",
      "\n",
      "   4. Original: charwoman\n",
      "      Enhanced: Female charwoman in Bedfordshire\n",
      "\n",
      "   5. Original: journeyman shoe maker\n",
      "      Enhanced: Male journeyman shoe maker in Bedfordshire\n",
      "\n",
      "\n",
      " ENHANCEMENT IMPACT ANALYSIS\n",
      "========================================\n",
      "   Length Analysis:\n",
      "   Original average length: 18.8 characters\n",
      "   Enhanced average length: 39.5 characters\n",
      "   Average increase: 20.7 characters (109.9%)\n",
      "\n",
      " Information Density:\n",
      "   Unique occupations: 872\n",
      "   Unique genders: 3\n",
      "   Unique counties: 56\n",
      "   Unique descriptions: 317\n",
      "   Average context elements per record: 3.99\n",
      "\n",
      " Expected HISCO Prediction Improvement:\n",
      "   Expected accuracy improvement: 20-30%\n",
      "   Confidence level: High\n",
      "\n",
      " Most Information-Rich Examples:\n",
      "   1. Male dressmaker in Anglesey\n",
      "   2. Male retired draper in Anglesey\n",
      "   3. Male clogger barber in Anglesey\n",
      "\n",
      " SAVING ENHANCED INPUTS FOR OCCANINE\n",
      "==================================================\n",
      " Enhanced inputs saved: ./cleaning_results_1851_validation\\enhanced_input_for_occanine.csv\n",
      "    Records: 996\n",
      "    Ready for OccCANINE processing\n",
      " Enhanced array saved: ./cleaning_results_1851_validation\\enhanced_input_array.npy\n",
      "    Array shape: (996,)\n",
      "    Load with: np.load('./cleaning_results_1851_validation\\enhanced_input_array.npy')\n",
      "\n",
      " ENHANCEMENT COMPLETE!\n",
      " Files saved in: ./cleaning_results_1851_validation\n",
      " Ready for OccCANINE testing\n",
      " Expected improvement: 20-30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\51591\\AppData\\Local\\Temp\\ipykernel_36192\\2690574879.py:69: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  has_sex = enhanced_strings.str.contains(r'^(Male|Female)', case=False, na=False).sum()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def create_enhanced_input_strings(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Create enhanced input strings in format: \"Sex final_result in County\"\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame with columns: Sex, final_result, Occode_Desc, County\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Enhanced input strings\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\" CREATING ENHANCED INPUT STRINGS\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"Format: 'Sex final_result in County'\")\n",
    "    \n",
    "    def format_enhanced_string(row):\n",
    "        \"\"\"\n",
    "        Create enhanced string: \"Sex final_result in County\"\n",
    "        Example: \"Male agricultural labourer (AGRICULTURAL LABOURER, FARM SERVANT) in ANGLESEY\"\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Get components\n",
    "            sex = str(row['Sex']).title() if pd.notna(row['Sex']) else \"\"\n",
    "            occupation = str(row['final_result']).lower() if pd.notna(row['final_result']) else \"\"\n",
    "            county = str(row['County']).title() if pd.notna(row['County']) else \"\"\n",
    "\n",
    "            \n",
    "            # Build the enhanced string\n",
    "            result_parts = []\n",
    "            \n",
    "            # Add sex\n",
    "            if sex:\n",
    "                result_parts.append(sex)\n",
    "            \n",
    "            result_parts.append(f\"{occupation}\")\n",
    "\n",
    "            # Join parts so far\n",
    "            base_string = \" \".join(result_parts)\n",
    "            \n",
    "            # Add location with \"in\"\n",
    "            if county:\n",
    "                if base_string:\n",
    "                    return f\"{base_string} in {county}\"\n",
    "                else:\n",
    "                    return f\"in {county}\"\n",
    "            else:\n",
    "                return base_string\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error formatting row: {e}\")\n",
    "            # Fallback to original occupation\n",
    "            return str(row.get('final_result', ''))\n",
    "    \n",
    "    # Apply formatting to all rows\n",
    "    enhanced_strings = df.apply(format_enhanced_string, axis=1)\n",
    "    \n",
    "    # Print statistics\n",
    "    print(f\"\\n Enhancement Statistics:\")\n",
    "    print(f\"   Total strings created: {len(enhanced_strings):,}\")\n",
    "    print(f\"   Average length: {enhanced_strings.str.len().mean():.1f} characters\")\n",
    "    print(f\"   Max length: {enhanced_strings.str.len().max()} characters\")\n",
    "    print(f\"   Min length: {enhanced_strings.str.len().min()} characters\")\n",
    "    \n",
    "    # Count how many strings have each component\n",
    "    has_sex = enhanced_strings.str.contains(r'^(Male|Female)', case=False, na=False).sum()\n",
    "    has_description = enhanced_strings.str.contains(r'\\([^)]+\\)', na=False).sum()\n",
    "    has_location = enhanced_strings.str.contains(r' in \\w+', case=False, na=False).sum()\n",
    "    \n",
    "    print(f\"\\n Component Coverage:\")\n",
    "    print(f\"   With gender: {has_sex:,} ({has_sex/len(enhanced_strings)*100:.1f}%)\")\n",
    "    print(f\"   With description: {has_description:,} ({has_description/len(enhanced_strings)*100:.1f}%)\")\n",
    "    print(f\"   With location: {has_location:,} ({has_location/len(enhanced_strings)*100:.1f}%)\")\n",
    "    \n",
    "    # Show sample results\n",
    "    print(f\"\\n Sample Enhanced Strings:\")\n",
    "    for i in range(min(5, len(enhanced_strings))):\n",
    "        original = df.iloc[i]['final_result'] if 'final_result' in df.columns else 'N/A'\n",
    "        enhanced = enhanced_strings.iloc[i]\n",
    "        print(f\"   {i+1}. Original: {original}\")\n",
    "        print(f\"      Enhanced: {enhanced}\")\n",
    "        print()\n",
    "    \n",
    "    return enhanced_strings\n",
    "\n",
    "def save_enhanced_inputs_for_occanine(df: pd.DataFrame, \n",
    "                                    enhanced_strings: pd.Series,\n",
    "                                    output_dir: str = \"./cleaning_results_1861\") -> str:\n",
    "    \"\"\"\n",
    "    Save enhanced input strings for OccCANINE testing\n",
    "    \n",
    "    Args:\n",
    "        df: Original DataFrame\n",
    "        enhanced_strings: Enhanced input strings\n",
    "        output_dir: Output directory\n",
    "    \n",
    "    Returns:\n",
    "        str: Path to saved file\n",
    "    \"\"\"\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n SAVING ENHANCED INPUTS FOR OCCANINE\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create output DataFrame for OccCANINE\n",
    "    occanine_input = pd.DataFrame({\n",
    "        'record_id': range(len(df)),\n",
    "        'original_hisco': df.get('HISCO_x', ''),\n",
    "        'enhanced_input': enhanced_strings,\n",
    "        'original_occupation': df.get('final_result', ''),\n",
    "        'sex': df.get('Sex', ''),\n",
    "        'county': df.get('County', ''),\n",
    "        'count': df.get('Count', 1)\n",
    "    })\n",
    "    \n",
    "    # Save enhanced input file\n",
    "    enhanced_file_path = os.path.join(output_dir, \"enhanced_input_for_occanine.csv\")\n",
    "    occanine_input.to_csv(enhanced_file_path, index=False)\n",
    "    \n",
    "    print(f\" Enhanced inputs saved: {enhanced_file_path}\")\n",
    "    print(f\"    Records: {len(occanine_input):,}\")\n",
    "    print(f\"    Ready for OccCANINE processing\")\n",
    "    \n",
    "    # Also save just the enhanced strings array (like your original final_result_array)\n",
    "    enhanced_array_path = os.path.join(output_dir, \"enhanced_input_array.npy\")\n",
    "    np.save(enhanced_array_path, enhanced_strings.to_numpy())\n",
    "    \n",
    "    print(f\" Enhanced array saved: {enhanced_array_path}\")\n",
    "    print(f\"    Array shape: {enhanced_strings.to_numpy().shape}\")\n",
    "    print(f\"    Load with: np.load('{enhanced_array_path}')\")\n",
    "    \n",
    "    return enhanced_file_path\n",
    "\n",
    "def analyze_enhancement_impact(df: pd.DataFrame, enhanced_strings: pd.Series):\n",
    "    \"\"\"\n",
    "    Analyze the impact of the enhancement on input strings\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"\\n ENHANCEMENT IMPACT ANALYSIS\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    original_strings = df['final_result'].astype(str)\n",
    "    \n",
    "    # Length comparison\n",
    "    orig_avg_length = original_strings.str.len().mean()\n",
    "    enhanced_avg_length = enhanced_strings.str.len().mean()\n",
    "    length_increase = enhanced_avg_length - orig_avg_length\n",
    "    \n",
    "    print(f\"   Length Analysis:\")\n",
    "    print(f\"   Original average length: {orig_avg_length:.1f} characters\")\n",
    "    print(f\"   Enhanced average length: {enhanced_avg_length:.1f} characters\")\n",
    "    print(f\"   Average increase: {length_increase:.1f} characters ({length_increase/orig_avg_length*100:.1f}%)\")\n",
    "    \n",
    "    # Information density analysis\n",
    "    print(f\"\\n Information Density:\")\n",
    "    \n",
    "    # Count unique information elements\n",
    "    unique_occupations = df['final_result'].nunique()\n",
    "    unique_sexes = df['Sex'].nunique() if 'Sex' in df.columns else 0\n",
    "    unique_counties = df['County'].nunique() if 'County' in df.columns else 0\n",
    "    unique_descriptions = df['Occode_Desc'].nunique() if 'Occode_Desc' in df.columns else 0\n",
    "    \n",
    "    print(f\"   Unique occupations: {unique_occupations:,}\")\n",
    "    print(f\"   Unique genders: {unique_sexes}\")\n",
    "    print(f\"   Unique counties: {unique_counties}\")\n",
    "    print(f\"   Unique descriptions: {unique_descriptions:,}\")\n",
    "    \n",
    "    # Context richness score\n",
    "    context_elements_per_record = 1  # Always have occupation\n",
    "    if 'Sex' in df.columns:\n",
    "        context_elements_per_record += (df['Sex'].notna()).mean()\n",
    "    if 'County' in df.columns:\n",
    "        context_elements_per_record += (df['County'].notna()).mean()\n",
    "    if 'Occode_Desc' in df.columns:\n",
    "        context_elements_per_record += (df['Occode_Desc'].notna()).mean()\n",
    "    \n",
    "    print(f\"   Average context elements per record: {context_elements_per_record:.2f}\")\n",
    "    \n",
    "    # Estimate improvement potential\n",
    "    print(f\"\\n Expected HISCO Prediction Improvement:\")\n",
    "    \n",
    "    if context_elements_per_record >= 3.5:\n",
    "        expected_improvement = \"20-30%\"\n",
    "        confidence = \"High\"\n",
    "    elif context_elements_per_record >= 3.0:\n",
    "        expected_improvement = \"15-25%\"\n",
    "        confidence = \"High\"\n",
    "    elif context_elements_per_record >= 2.5:\n",
    "        expected_improvement = \"10-18%\"\n",
    "        confidence = \"Medium-High\"\n",
    "    elif context_elements_per_record >= 2.0:\n",
    "        expected_improvement = \"5-12%\"\n",
    "        confidence = \"Medium\"\n",
    "    else:\n",
    "        expected_improvement = \"2-8%\"\n",
    "        confidence = \"Low-Medium\"\n",
    "    \n",
    "    print(f\"   Expected accuracy improvement: {expected_improvement}\")\n",
    "    print(f\"   Confidence level: {confidence}\")\n",
    "    \n",
    "    # Show most informative examples\n",
    "    print(f\"\\n Most Information-Rich Examples:\")\n",
    "    # Find records with all components\n",
    "    complete_records = df[\n",
    "        df['Sex'].notna() & \n",
    "        df['County'].notna() & \n",
    "        df['Occode_Desc'].notna() &\n",
    "        df['final_result'].notna()\n",
    "    ]\n",
    "    \n",
    "    if len(complete_records) > 0:\n",
    "        for i in range(min(3, len(complete_records))):\n",
    "            idx = complete_records.index[i]\n",
    "            print(f\"   {i+1}. {enhanced_strings.iloc[idx]}\")\n",
    "    \n",
    "    return {\n",
    "        'original_avg_length': orig_avg_length,\n",
    "        'enhanced_avg_length': enhanced_avg_length,\n",
    "        'length_increase_pct': length_increase/orig_avg_length*100,\n",
    "        'context_elements': context_elements_per_record,\n",
    "        'expected_improvement': expected_improvement,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "def enhance_occupation_inputs(df_predict: pd.DataFrame, \n",
    "                            output_dir: str = \"./cleaning_results_1881\") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Main function to create enhanced input strings for OccCANINE\n",
    "    Format: \"Sex final_result (Occode_Desc) in County\"\n",
    "    \n",
    "    Args:\n",
    "        df_predict: DataFrame with your aggregated occupation data\n",
    "        output_dir: Directory to save output files\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Enhanced input strings (equivalent to your final_result_array)\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\" ENHANCING OCCUPATION INPUTS FOR OCCANINE\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Validate required columns\n",
    "    required_cols = ['final_result']\n",
    "    missing_cols = [col for col in required_cols if col not in df_predict.columns]\n",
    "    \n",
    "    if missing_cols:\n",
    "        raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "    \n",
    "    # Check optional columns\n",
    "    optional_cols = ['Sex', 'County', 'HISCO_x']\n",
    "    available_cols = [col for col in optional_cols if col in df_predict.columns]\n",
    "    \n",
    "    print(f\" Required columns found: {required_cols}\")\n",
    "    print(f\" Available optional columns: {available_cols}\")\n",
    "    \n",
    "    if not available_cols:\n",
    "        print(\"  Warning: No optional columns found. Enhancement will be limited.\")\n",
    "    \n",
    "    # Create enhanced input strings\n",
    "    enhanced_strings = create_enhanced_input_strings(df_predict)\n",
    "    \n",
    "    # Analyze enhancement impact\n",
    "    impact_analysis = analyze_enhancement_impact(df_predict, enhanced_strings)\n",
    "    \n",
    "    # Save for OccCANINE\n",
    "    output_file = save_enhanced_inputs_for_occanine(df_predict, enhanced_strings, output_dir)\n",
    "    \n",
    "    print(f\"\\n ENHANCEMENT COMPLETE!\")\n",
    "    print(f\" Files saved in: {output_dir}\")\n",
    "    print(f\" Ready for OccCANINE testing\")\n",
    "    print(f\" Expected improvement: {impact_analysis['expected_improvement']}\")\n",
    "    \n",
    "    # Return the enhanced strings array (like your original final_result_array)\n",
    "    return enhanced_strings\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    df_predict = pd.read_csv(\".\\\\cleaning_results_1851_validation\\\\aggregated_occupations.csv\", header=0)\n",
    "    enhanced_input_array = enhance_occupation_inputs(df_predict, output_dir = \"./cleaning_results_1851_validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OccCANINE predict HISCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\hisco\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Anaconda\\envs\\hisco\\Lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] 找不到指定的程序。'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch 4 out of 4 batchesProduced HISCO codes for 996 observations in 0 hours, 0 minutes and 1.331 seconds.\n",
      "Estimated hours saved compared to human labeller (assuming 10 seconds per label):\n",
      " ---> 2 hours, 45 minutes and 59 seconds\n"
     ]
    }
   ],
   "source": [
    "from histocc import OccCANINE\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "model = OccCANINE()\n",
    "model_new = OccCANINE(\"Finetuned/finetune_model\",hf = False)\n",
    "enhanced_input_array = np.load(\"D:\\\\Postgraduate\\\\Data Science Project\\\\Code\\\\cleaning_results_1851_validation\\\\enhanced_input_array.npy\",allow_pickle=True)\n",
    "\n",
    "df_predict = pd.read_csv(\".\\\\cleaning_results_1851_validation\\\\aggregated_occupations.csv\", header=0)\n",
    "\n",
    "\n",
    "raw_input_array = df_predict[\"Occupation_String\"]\n",
    "standardized_input_array = df_predict[\"final_result\"]\n",
    "\n",
    "\n",
    "# no finetune model\n",
    "raw_input = model.predict(\n",
    "    raw_input_array,\n",
    "    get_dict = True,\n",
    "    lang = \"en\"\n",
    ")\n",
    "\n",
    "standardized_input = model.predict(\n",
    "    standardized_input_array,\n",
    "    get_dict = True,\n",
    "    lang = \"en\"\n",
    ")\n",
    "\n",
    "context_input = model.predict(\n",
    "    enhanced_input_array,\n",
    "    get_dict = True,\n",
    "    lang = \"en\"\n",
    ")\n",
    "\n",
    "# Finetuned model\n",
    "model_new.verbose = True\n",
    "# context input\n",
    "context_input_finetune = model_new.predict(\n",
    "    enhanced_input_array,\n",
    "    get_dict = True,\n",
    "    lang = \"en\"\n",
    ")\n",
    "\n",
    "df_predict[\"OccCANINE_raw_input\"] = raw_input\n",
    "df_predict[\"OccCANINE_standardized_input\"] = standardized_input\n",
    "df_predict[\"OccCANINE_context_input\"] = context_input\n",
    "df_predict[\"OccCANINE_finetuned_context_input\"] = context_input_finetune\n",
    "import numpy as np\n",
    "\n",
    "finetune_series = pd.Series(context_input_finetune)\n",
    "context_series = pd.Series(context_input)\n",
    "raw_series = pd.Series(raw_input)\n",
    "\n",
    "# find []\n",
    "def is_invalid(val):\n",
    "    if val is None:\n",
    "        return True\n",
    "    if isinstance(val, float) and np.isnan(val):\n",
    "        return True\n",
    "    if isinstance(val, str) and (val.strip() == \"\" or val.strip() == \"[]\"):\n",
    "        return True\n",
    "    if isinstance(val, (list, np.ndarray)) and len(val) == 0:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# use context_input fill in finetuned\n",
    "final_result = finetune_series.copy()\n",
    "mask_invalid = finetune_series.apply(is_invalid)\n",
    "final_result[mask_invalid] = context_series[mask_invalid]\n",
    "\n",
    "# rest part use raw_input to fill nan\n",
    "mask_still_invalid = final_result.apply(is_invalid)\n",
    "final_result[mask_still_invalid] = raw_series[mask_still_invalid]\n",
    "\n",
    "df_predict[\"OccCANINE_final\"] = final_result\n",
    "\n",
    "df_predict.to_csv(\".\\\\cleaning_results_1851_validation\\\\OccCANINE_finetune_result.csv\",index=False)\n",
    "\n",
    "merged_df = pd.read_csv(\"D:\\\\Postgraduate\\\\Data Science Project\\\\Code\\\\cleaning_results_1851_validation\\\\OccCANINE_result_multiple.csv\",header=0)\n",
    "multi_df = merged_df[['Occupation_String', 'all_predicted_hisco', 'prediction_count', 'prediction_details']]\n",
    "all_result_merge = pd.merge(df_predict,multi_df,how='left',on='Occupation_String')\n",
    "all_result_merge.to_csv(\".\\\\cleaning_results_1851_validation\\\\OccCANINE_result_merged.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\".\\\\cleaning_results_1911\\\\OccCANINE_result_merged.csv\", header=0,encoding=\"latin1\")\n",
    "df_val = df.sample(n=100, random_state=42)\n",
    "\n",
    "df_val.to_csv(\".\\\\1911_validation_sample_auto.csv\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hisco",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
